# slambook

## 视觉里程计

### 直接法

#### 直接法的引入

特征点法在视觉里程计中占据主流地位，研究者们认识到它至少有以下几个缺点： 

1. 关键点的提取与描述子的计算非常耗时。实践当中，SIFT目前在CPU上是无法实时计算的，而ORB也需要近20毫秒的计算。如果整个SLAM以30毫秒/帧的速度运行，那么一大半时间都花在计算特征点上。 

2. 使用特征点时，忽略了除特征点以外的所有信息。一张图像有几十万个像素，而特征点只有几百个。只使用特征点丢弃了大部分可能有用的图像信息。 

3. 相机有时会运动到特征缺失的地方，往往这些地方没有明显的纹理信息。例如，有时我们会面对一堵白墙，或者一个空荡荡的走廓。这些场景下特征点数量会明显减少，我们可能找不到足够的匹配点来计算相机运动。 

我们看到使用特征点确实存在一些问题。有没有什么办法能够克服这些缺点呢？我们有以下几种思路： 

- 保留特征点，但只计算关键点，不计算描述子。同时，使用光流法（Optical Flow）来跟踪特征点的运动。这样可以回避计算和匹配描述子带来的时间，但光流本身的计算需要一定时间。 

- 只计算关键点，不计算描述子。同时，使用直接法（Direct Method）来计算特征点在下一时刻图像的位置。这同样可以跳过描述子的计算过程，而且直接法的计算更加简单。 

- 既不计算关键点、也不计算描述子，而是根据像素灰度的差异，直接计算相机运动。 

第一种方法仍然使用特征点，只是把匹配描述子替换成了光流跟踪，估计相机运动时仍使用对极几何、PnP或ICP算法。而在后两个方法中，我们会根据图像的像素灰度信息来计算相机运动，它们都称为直接法。 

使用特征点法估计相机运动时，我们把特征点看作固定在三维空间的不动点。根据它们在相机中的投影位置，通过最小化重投影误差（Reprojection error）来优化相机运动。在这个过程中，我们需要精确地知道空间点在两个相机中投影后的像素位置——这也就是我们为何要对特征进行匹配或跟踪的理由。同时，我们也知道，计算、匹配特征需要付出大量的计算量。相对的，在直接法中，我们并不需要知道点与点之间之间的对应关系，而是通过最小化光度误差（Photometric error）来求得它们。 

直接法是为了克服特征点法的上述缺点而存在的。直接法根据像素的亮度信息，估计相机的运动，可以完全不用计算关键点和描述子，于是，既避免了特征的计算时间，也避免了特征缺失的情况。只要场景中存在明暗变化（可以是渐变，不形成局部的图像梯度），直接法就能工作。根据使用像素的数量，直接法分为稀疏、稠密和半稠密三种。相比于特征点法只能重构稀疏特征点（稀疏地图），直接法还具有恢复稠密或半稠密结构的能力。 

历史上，虽然早期也有一些对直接法的使用，但直到RGB-D相机出现后，人们才发现直接法对RGB-D相机，进而对于单目相机，都是行之有效的方法。随着一些使用直接法的开源项目的出现（如SVO、LSD-SLAM等），它们逐渐地走上主流舞台，成为视觉里程计算法中重要的一部分。

#### 直接法的推导

考虑某个空间点$P$和两个时刻的相机。$P$的世界坐标为$[X, Y, Z]$，它在两个相机上成像，记非齐次像素坐标为$p_1$、$p_2$ 。我们的目标是求第一个相机到第二个相机的相对位姿变换。我们以第一个相机为参照系，设第二个相机旋转和平移为$R$、$t$（对应李代数为$\xi$）。同时，两相机的内参相同，记为$K$。为清楚起见，我们列写完整的投影方程：

$p_1=\begin{bmatrix}u\\v\\1\end{bmatrix}_1=\frac{1}{Z_1}KP$

$p_2=\begin{bmatrix}u\\v\\1\end{bmatrix}_2=\frac{1}{Z_2}K(RP+t)=\frac{1}{Z_2}(\exp(\xi^{\wedge})P)_{1:3}$

其中$Z_1$是$P$的深度，$Z_2$是$P$在第二个相机坐标系下的深度，也就是$RP+t$的第三个坐标值。由于$\exp(\xi^{\wedge})$只能和齐次坐标相乘，所以我们乘完之后要取出前三个元素。

<img title="" src="slambook/2025-04-10-09-33-27-image.png" alt="" width="428">

回忆特征点法中，由于我们通过匹配描述子，知道了$p_1$、$p_2$的像素位置，所以可以计算重投影的位置。但在直接法中，由于没有特征匹配，我们无从知道哪一个$p_2$与$p_1$对应着同一个点。直接法的思路是根据当前相机的位姿估计值，来寻找$p_2$的位置。但若相机位姿不够好，$p_2$的外观和$p_1$会有明显差别。于是，为了减小这个差别，我们优化相机的位姿，来寻找与$p_1$更相似的$p_2$。这同样可以通过解一个优化问题，但此时最小化的不是重投影误差，而是光度误差（Photometric Error），也就是$P$的两个像的亮度误差：

$e=I_1(p_1)-I_2(p_2)$

注意这里$e$是一个标量。同样的，优化目标为该误差的二范数，暂时取不加权的形式，为：

$\min\limits_{\xi}J(\xi)=\Vert e\Vert^2$

能够做这种优化的理由，仍是基于灰度不变假设。在直接法中，我们假设一个空间点在各个视角下，成像的灰度是不变的。我们有许多个（比如$N$个）空间点$P_i$，那么，整个相机位姿估计问题变为：

$\min\limits_{\xi}J(\xi)=\sum\limits^{N}_{i=1}e^T_ie_i,\quad e_i=I_1(p_1,i)-I_2(p_2,i)$

注意这里的优化变量是相机位姿 $\xi$。为了求解这个优化问题，我们关心误差$e$是如何随着相机位姿$\xi$变化的，需要分析它们的导数关系。因此，使用李代数上的扰动模型。我们给$\exp(\xi)$左乘一个小扰动$\exp(\xi)$，得：

$\begin{aligned}e(\xi\oplus\delta\xi)&=I_1(\frac{1}{Z_1}KP)-I_2(\frac{1}{Z_2}K\exp(\delta\xi^{\wedge})\exp(\xi^{\wedge})P)\\&\approx I_1(\frac{1}{Z_1}KP)-I_2(\frac{1}{Z_2}K(1+\delta\xi^{\wedge})\exp(\xi^{\wedge})P)\\&=I_1(\frac{1}{Z_1}KP)-I_2(\frac{1}{Z_2}K\exp(\xi^{\wedge})P+\frac{1}{Z_2}K\delta\xi^{\wedge}\exp(\xi^{\wedge})P)\end{aligned}$

记：

$q=\delta\xi^{\wedge}\exp(\xi^{\wedge})P$

$u=\frac{1}{Z_2}Kq$

这里的$q$为$P$在扰动之后，位于第二个相机坐标系下的坐标，而$u$为它的像素坐标。利用一阶泰勒展开，有：

$\begin{aligned}e(\xi\oplus\delta\xi)&=I_1(\frac{1}{Z_1}KP)-I_2(\frac{1}{Z_2}K\exp(\xi^{\wedge})P+u)\\&\approx I_1(\frac{1}{Z_1}KP)-I_2(\frac{1}{Z_2}K\exp(\xi^{\wedge})P)-\frac{\partial I_2}{\partial u}\frac{u}{\partial q}\frac{\partial q}{\partial\delta\xi}\delta\xi\\&=e(\xi)-\frac{\partial I_2}{\partial u}\frac{u}{\partial q}\frac{\partial q}{\partial\delta\xi}\delta\xi\end{aligned}$

我们看到，一阶导数由于链式法则分成了三项，而这三项都是容易计算的：

1. $\frac{\partial I_2}{\partial u}$为$u$处的像素梯度；

2. $\frac{\partial u}{\partial q}$为投影方程关于相机坐标系下的三维点的导数。记$q=[X,Y,Z]^T$，根据上一
   节的推导，导数为：
   
   $\frac{\partial u}{\partial q}=\begin{bmatrix}\frac{\partial u}{\partial X}&\frac{\partial u}{\partial Y}&\frac{\partial u}{\partial Z}\\\frac{\partial v}{\partial X}&\frac{\partial v}{\partial Y}&\frac{\partial v}{\partial Z}\end{bmatrix}=\begin{bmatrix}\frac{f_x}{Z}&0&-\frac{f_xX}{Z^2}\\0&\frac{f_y}{Z}&-\frac{f_yY}{Z^2}\end{bmatrix}$

3. $\frac{\partial q}{\partial\delta\xi}$为变换后的三维点对变换的导数：
   
   $\frac{\partial q}{\partial\delta\xi}=[I,-q^{\wedge}]$

在实践中，由于后两项只与三维点q有关，而与图像无关，我们经常把它合并在一起：

$\frac{\partial u}{\partial\delta\xi}=\begin{bmatrix}\frac{f_x}{Z}&0&-\frac{f_xX}{Z^2}&-\frac{f_xXY}{Z^2}&f_x+\frac{f_xX^2}{Z^2}&-\frac{f_xY}{Z^2}\\0&\frac{f_y}{Z}&-\frac{f_yY}{Z^2}&-f_y-\frac{f_yY^2}{Z^2}&\frac{f_yXY}{Z^2}&\frac{f_yX}{Z}\end{bmatrix}$

于是，我们推导了误差相对于李代数的雅可比矩阵：

$J=-\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial\delta\xi}$

对于$N$个点的问题，我们可以用这种方法计算优化问题的雅可比，然后使用G-N或L-M计算增量，迭代求解。至此，我们推导了直接法估计相机位姿的整个流程，下面我们通过程序来演示一下直接法是如何使用的。

#### 直接法的讨论

在上面的推导中，$P$是一个已知位置的空间点，它是怎么来的呢？在RGB-D相机下，我们可以把任意像素反投影到三维空间，然后投影到下一张图像中。如果在单目相机中，这件事情要更为困难，因为我们还需考虑由P的深度带来的不确定性。

根据$P$的来源，我们可以把直接法进行分类：

1. $P$来自于稀疏关键点，我们称之为稀疏直接法。通常我们使用数百个至上千个关键点，并且像L-K光流那样，假设它周围像素也是不变的。这种稀疏直接法不必计算描述子，并且只使用数百个像素，因此速度最快，但只能计算稀疏的重构。
2. $P$来自部分像素。如果像素梯度为零，整一项雅可比就为零，不会对计算运动增量有任何贡献。因此，可以考虑只使用带有梯度的像素点，舍弃像素梯度不明显的地方。这称之为半稠密（Semi-Dense）的直接法，可以重构一个半稠密结构。
3. $P$为所有像素，称为稠密直接法。稠密重构需要计算所有像素（一般几十万至几百万个），因此多数不能在现有的CPU上实时计算，需要GPU的加速。但是，如前面所讨论的，梯度不明显的点，在运动估计中不会有太大贡献，在重构时也会难以估计位置。

可以看到，从稀疏到稠密重构，都可以用直接法来计算。它们的计算量是逐渐增长的。稀疏方法可以快速地求解相机位姿，而稠密方法可以建立完整地图。具体使用哪种方法，需要视机器人的应用环境而定。特别地，在低端的计算平台上，稀疏直接法可以做到非常快速的效果，适用于实时性较高且计算资源有限的场合。

### 光流

直接法是从光流演变而来的。它们非常相似，具有相同的假设条件。光流描述了像素在图像中的运动，而直接法则附带着一个相机运动模型。

光流是一种描述像素随着时间，在图像之间运动的方法，如下图所示。随着时间的经过，同一个像素会在图像中运动，而我们希望追踪它的运动过程。计算部分像素运动的称为稀疏光流，计算所有像素的称为稠密光流。稀疏光流以Lucas-Kanade 光流为代表，并可以在SLAM中用于跟踪特征点位置。因此，本节主要介绍Lucas-Kanade光流，亦称LK光流。

<img src="slambook/2025-04-12-13-56-00-image.png" title="" alt="" width="559">

#### Lucas-Kanade光流

在LK光流中，我们认为来自相机的图像是随时间变化的。图像可以看作时间的函数：$I(t)$。那么，一个在$t$时刻，位于$(x,y)$处的像素，它的灰度可以写成：$I(x,y,t)$。 

这种方式把图像看成了关于位置与时间的函数，它的值域就是图像中像素的灰度。现在考虑某个固定的空间点，它在t时刻的像素坐标为$(x,y)$。由于相机的运动，它的图像坐标将发生变化。我们希望估计这个空间点在其他时刻里图像的位置，这里要引入光流法的基本假设： 

灰度不变假设：同一个空间点的像素灰度值，在各个图像中是固定不变的。 

对于t时刻位于$(x,y)$处的像素，我们设t+dt时刻，它运动到$(x+{\rm d}x,y+{\rm d}y)$处。由于灰度不变，我们有：

$I(x+{\rm d}x,y+{\rm d}y,t+{\rm d}t)=I(x,y,t)$

灰度不变假设是一个很强的假设，实际当中很可能不成立。事实上，由于物体的材质不同，像素会出现高光和阴影部分；有时，相机会自动调整曝光参数，使得图像整体变亮或变暗。这些时候灰度不变假设都是不成立的，因此光流的结果也不一定可靠。然而，从另一方面来说，所有算法都是在一定假设下工作的。如果我们什么假设都不做，就没法设计实用的算法。所以，暂且让我们认为该假设成立，看看如何计算像素的运动。 

对左边进行泰勒展开，保留一阶项，得：

$I(x+{\rm d}x,y+{\rm d}y,t+{\rm d}t)\approx I(x,y,t)+\frac{\partial I}{\partial x}{\rm d}x+\frac{\partial I}{\partial y}{\rm d}y+\frac{\partial I}{\partial t}{\rm d}t$

因为我们假设了灰度不变，于是下一个时刻的灰度等于之前的灰度，从而：

$\frac{\partial I}{\partial x}{\rm d}x+\frac{\partial I}{\partial y}{\rm d}y+\frac{\partial I}{\partial t}{\rm d}t=0$

两边除以${\rm d}t$，得：

$\frac{\partial I}{\partial x}\frac{{\rm d}x}{{\rm d}t}+\frac{\partial I}{\partial y}\frac{{\rm d}y}{{\rm d}t}=-\frac{\partial I}{\partial t}$

其中$\frac{{\rm d}x}{{\rm d}t}$为像素在$x$轴上运动速度，而$\frac{{\rm d}y}{{\rm d}t}$为$y$轴速度，把它们记为$u$、$v$。同时$\frac{\partial I}{\partial x}$为图像在该点处$x$方向的梯度，另一项则是在$y$方向的梯度，记为$I_x$、$I_y$。把图像灰度对时间的变化量记为$I_t$，写成矩阵形式，有：

$\begin{bmatrix}I_x&I_y\end{bmatrix}\begin{bmatrix}u\\v\end{bmatrix}=-I_t$

我们想计算的是像素的运动$u$、$v$，但是该式是带有两个变量的一次方程，仅凭它无法计算出$u$、$v$。因此，必须引入额外的约束来计算$u$、$v$。在LK光流中，我们假设某一个窗口内的像素具有相同的运动。 

考虑一个大小为$w\times w$大小的窗口，它含有$w^2$数量的像素。由于该窗口内像素具有同样的运动，因此我们共有$w^2$个方程：

$\begin{bmatrix}I_x&I_y\end{bmatrix}_k\begin{bmatrix}u\\v\end{bmatrix}=-I_{tk},\quad k=1,...,w^2$

记：

$A=\begin{bmatrix}\begin{bmatrix}I_x&I_y\end{bmatrix}_1\\\vdots\\\begin{bmatrix}I_x&I_y\end{bmatrix}_k\end{bmatrix},b=\begin{bmatrix}I_{t1}\\\vdots\\I_{tk}\end{bmatrix}$

于是整个方程为：

$A\begin{bmatrix}u\\v\end{bmatrix}=-b$

这是一个关于$u$、$v$的超定线性方程，传统解法是求最小二乘解：

$\begin{bmatrix}u\\v\end{bmatrix}^*=-(A^TA)^{-1}A^Tb$

这样就得到了像素在图像间的运动速度$u$、$v$。当t取离散的时刻而不是连续时间时，我们可以估计某块像素在若干个图像中出现的位置。由于像素梯度仅在局部有效，所以如果一次迭代不够好的话，我们会多迭代几次这个方程。在SLAM中，LK光流常被用来跟踪角点的运动。

### 特征点法

视觉SLAM主要分为视觉前端和优化后端。前端也称为视觉里程计（VO）。它根据相邻图像的信息，估计出粗略的相机运动，给后端提供较好的初始值。VO的实现方法，按是否需要提取特征，分为特征点法的前端以及不提特征的直接法前端。基于特征点法的前端，长久以来（直到现在）被认为是视觉里程计的主流方法。它运行稳定，对光照、动态物体不敏感，是目前比较成熟的解决方案。一个基于特征点法的视觉里程计通过提取、匹配图像特征点，然后估计两帧之间的相机运动和场景结构。

#### 特征点

VO的主要问题是如何根据图像来估计相机运动。然而，图像本身是一个由亮度和色彩组成的矩阵，如果直接从矩阵层面考虑运动估计，将会非常困难。所以，我们习惯于采用这样一种做法：首先，从图像中选取比较有代表性的点。这些点在相机视角发生少量变化后会保持不变，所以我们会在各个图像中找到相同的点。然后，在这些点的基础上，讨论相机位姿估计问题，以及这些点的定位问题。在经典SLAM模型中，把它们称为路标。而在视觉SLAM中，路标则是指图像特征（Features）。 

根据维基百科的定义，图像特征是一组与计算任务相关的信息，计算任务取决于具体的应用。简而言之，特征是图像信息的另一种数字表达形式。一组好的特征对于在指定任务上的最终表现至关重要，所以多年来研究者们花费了大量的精力对特征进行研究。数字图像在计算机中以灰度值矩阵的方式存储，所以最简单的，单个图像像素也是一种"特征"。但是，在视觉里程计中，我们希望特征点在相机运动之后保持稳定，而灰度值受光照、形变、物体材质的影响严重，在不同图像之间变化非常大，不够稳定。理想的情况是，当场景和相机视角发生少量改变时，我还能从图像中判断哪些地方是同一个点，因此仅凭灰度值是不够的，我们需要对图像提取特征点。 

特征点是图像里一些特别的地方。以下图为例。我们可以把图像中的角点、边缘和区块都当成图像中有代表性的地方。不过，我们更容易精确地指出，某两幅图像当中出现了同一个角点；同一个边缘则稍微困难一些，因为沿着该边缘前进，图像局部是相似的；同一个区块则是最困难的。我们发现，图像中的角点、边缘相比于像素区块而言更加"特别"，它们不同图像之间的辨识度更强。所以，一种直观的提取特征的方式就是在不同图像间辨认角点，确定它们的对应关系。在这种做法中，角点就是所谓的特征。

<img src="slambook/2025-04-12-17-49-54-image.png" title="" alt="" width="373">

然而，在大多数应用中，单纯的角点依然不能满足很多我们的需求。例如，从远处看上去是角点的地方，当相机走近之后，可能就不显示为角点了。或者，当我旋转相机时，角点的外观会发生变化，我们也就不容易辨认出那是同一个角点。为此，计算机视觉领域的研究者们在长年的研究中，设计了许多更加稳定的局部图像特征，如著名的SIFT、SURF、ORB等。相比于朴素的角点，这些人工设计的特征点能够拥有如下的性质： 

1. 可重复性（Repeatability）：相同的"区域"可以在不同的图像中被找到。 

2. 可区别性（Distinctiveness）：不同的"区域"有不同的表达。 

3. 高效率（Efficiency）：同一图像中，特征点的数量应远小于像素的数量。 

4. 本地性（Locality）：特征仅与一小片图像区域相关。 

特征点由关键点（Key-point）和描述子（Descriptor）两部分组成。比方说，当我们谈论SIFT特征时，是指"提取SIFT关键点，并计算SIFT描述子"两件事情。关键点是指该特征点在图像里的位置，有些特征点还具有朝向、大小等信息。描述子通常是一个向量，按照某种人为设计的方式，描述了该关键点周围像素的信息。描述子是按照"外观相似的特征应该有相似的描述子"的原则设计的。因此，只要两个特征点的描述子在向量空间上的距离相近，就可以认为它们是同样的特征点。 

历史上，研究者提出过许多图像特征。它们有些很精确，在相机的运动和光照变化下仍具有相似的表达，但相应地需要较大的计算量。其中，SIFT（尺度不变特征变换，Scale-Invariant Feature Transform）当属最为经典的一种。它充分考虑了在图像变换过程中出现的光照、尺度、旋转等变化，但随之而来的是极大的计算量。由于整个SLAM过程中，图像特征的提取与匹配仅仅是诸多环节中的一个，到目前（2016年）为止，普通PC的CPU还无法实时地计算SIFT特征，进行定位与建图。所以在SLAM中我们甚少使用这种"奢侈"的图像特征。 

另一些特征，则考虑适当降低精度和鲁棒性，提升计算的速度。例如FAST关键点属于计算特别快的一种特征点（注意这里"关键点"的用词，说明它没有描述子）。而ORB（Oriented FAST and Rotated BRIEF）特征则是目前看来非常具有代表性的实时图像特征。它改进了FAST检测子不具有方向性的问题，并采用速度极快的二进制描述子BRIEF，使整个图像特征提取的环节大大加速。根据作者在论文中的测试，在同一幅图像中同时提取约1000个特征点的情况下，ORB约要花费15.3ms，SURF约花费217.3ms，SIFT约花费5228.7ms。由此可以看出ORB在保持了特征子具有旋转，尺度不变性的同 时，速度方面提升明显，对于实时性要求很高的SLAM来说是一个很好的选择。 

大部分特征提取都具有较好的并行性，可以通过GPU等设备来加速计算。经过GPU加速后的SIFT，就可以满足实时计算要求。但是，引入GPU将带来整个SLAM成本的提升。由此带来的性能提升，是否足以抵去付出的计算成本，需要系统的设计人员仔细考量。在目前的SLAM方案中，ORB是质量与性能之间较好的折中。

#### ORB特征

ORB特征亦由关键点和描述子两部分组成。它的关键点称为"Oriented FAST"，是一种改进的FAST角点，什么是FAST角点我们将在下文介绍。它的描述子称为BRIEF（Binary Robust Independent Elementary Features）。因此，提取ORB特征分为两个步骤： 

1. FAST角点提取：找出图像中的"角点"。相较于原版的FAST，ORB中计算了特征点的主方向，为后续的BRIEF描述子增加了旋转不变特性。 

2. BRIEF描述子：对前一步提取出特征点的周围图像区域进行描述。

![](slambook/52d53e1190872ee8f77de2a6586684dd1d9f3e12.png)

##### FAST关键点

FAST是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与它邻域的像素差别较大（过亮或过暗）, 那它更可能是角点。相比于其它角点检测算法，FAST只需比较像素亮度的大小，十分快捷。它的检测过程如下： 

1. 在图像中选取像素$p$，假设它的亮度为$I_p$。 

2. 设置一个阈值$T$（比如$I_p$的$20\%$）。 

3. 以像素$p$为中心, 选取半径为$3$的圆上的$16$个像素点。 

4. 假如选取的圆上，有连续的$N$个点的亮度大于$I_p+T$或小于$I_p−T$，那么像素$p$可以被认为是特征点（$N$通常取$12$，即为FAST-12。其它常用的$N$取值为$9$和$11$，他们分别被称为FAST-9，FAST-11）。 

5. 循环以上四步，对每一个像素执行相同的操作。

![](slambook/2025-04-13-10-27-49-image.png)

在FAST-12算法中，为了更高效，可以添加一项预测试操作，以快速地排除绝大多数不是角点的像素。具体操作为，对于每个像素，直接检测邻域圆上的第$1$、$5$、$9$、$13$个像素的亮度。只有当这四个像素中有三个同时大于$I_p+T$或小于$I_p−T$时，当前像素才有可能是一个角点，否则应该直接排除。这样的预测试操作大大加速了角点检测。此外，原始的FAST角点经常出现"扎堆"的现象。所以在第一遍检测之后，还需要用非极大值抑制（Non-maximal suppression），在一定区域内仅保留响应极大值的角点，避免角点集中的问题。 

FAST特征点的计算仅仅是比较像素间亮度的差异，速度非常快，但它也有一些问题。首先，FAST特征点数量很大且不确定，而我们往往希望对图像提取固定数量的特征。因此，在ORB中，对原始的FAST算法进行了改进。我们可以指定最终要提取的角点数量$N$，对原始FAST角点分别计算Harris响应值，然后选取前$N$个具有最大响应值的角点，作为最终的角点集合。 

其次，FAST角点不具有方向信息。而且，由于它固定取半径为$3$的圆，存在尺度问题： 

远处看着像是角点的地方，接近后看可能就不是角点了。针对FAST角点不具有方向性和尺度的弱点，ORB添加了尺度和旋转的描述。尺度不变性由构建图像金字塔，并在金字塔的每一层上检测角点来实现。而特征的旋转是由灰度质心法（Intensity Centroid）实现的。 

质心是指以图像块灰度值作为权重的中心。其具体操作步骤如下：

1. 在一个小的图像块B中，定义图像块的矩为：
   
   $m_{pq}=\sum\limits_{x,y\in B}x^py^qI(x,y),\quad p,q=\{0,1\}$

2. 通过矩可以找到图像块的质心：
   
   $C=(\frac{m_{10}}{m_{00}},\frac{m_{01}}{m_{00}})$

3. 连接图像块的几何中心$O$与质心$C$，得到一个方向向量$OC$，于是特征点的方向可以定义为：
   
   $\theta=\arctan(\frac{m_{01}}{m_{10}})$

通过以上方法，FAST角点便具有了尺度与旋转的描述，大大提升了它们在不同图像之间表述的鲁棒性。所以在ORB中，把这种改进后的FAST称为Oriented FAST。 

BRIEF描述子： 

在提取Oriented FAST关键点后，我们对每个点计算其描述子。ORB使用改进的BRIEF特征描述。 

BRIEF是一种二进制描述子，它的描述向量由许多个$0$和$1$组成，这里的$0$和$1$编码了关键点附近两个像素（比如说$p$和$q$）的大小关系：如果$p$比$q$大，则取$1$，反之就取$0$。如果我们取了$128$个这样的$p$、$q$，最后就得到$128$维由$0$、$1$组成的向量。那么，$p$和$q$如何选取呢？在作者原始的论文中给出了若干种挑选方法，大体上都是按照某种概率分布，随机地挑选$p$和$q$的位置。BRIEF使用了随机选点的比较，速度非常快，而且由于使用了二进制表达，存储起来也十分方便，适用于实时的图像匹配。原始的BRIEF描述子不具有旋转不变性的，因此在图像发生旋转时容易丢失。而ORB在FAST特征点提取阶段计算了关键点的方向，所以可以利用方向信息，计算了旋转之后的"Steer BRIEF"特征，使ORB的描述子具有较好的旋转不变性。

由于考虑到了旋转和缩放，使得ORB在平移、旋转、缩放的变换下仍有良好的表现。 同时，FAST和BRIEF的组合也非常的高效，使得ORB特征在实时SLAM中非常受欢迎。 

##### 特征匹配

特征匹配是视觉SLAM中极为关键的一步，宽泛地说，特征匹配解决了SLAM中的数据关联问题（data association），即确定当前看到的路标与之前看到的路标之间的对应关系。通过对图像与图像，或者图像与地图之间的描述子进行准确的匹配，我们可以为后续的姿态估计，优化等操作减轻大量负担。然而，由于图像特征的局部特性，误匹配的情况广泛存在，而且长期以来一直没有得到有效解决，目前已经成为视觉SLAM中制约性能提升的一大瓶颈。部分原因是因为场景中经常存在大量的重复纹理，使得特征描述非常相似。在这种情况下，仅利用局部特征解决误匹配是非常困难的。

![](slambook/2025-04-13-11-20-15-image.png)

不过，让我们先来看正确匹配的情况，等做完实验再回头去讨论误匹配问题。考虑两个时刻的图像。如果在图像$I_t$中提取到特征点$x^m_t,\quad m=1,2,...,M$，在图像$I_t+1$中提取到特征点$x^n_{t+1},\quad n=1,2,...,N$，如何寻找这两个集合元素的对应关系呢？最简单的特征匹配方法就是暴力匹配（Brute-Force Matcher）。即对每一个特征点$x^m_t$，与所有的$x^n_{t+1}$测量描述子的距离，然后排序，取最近的一个作为匹配点。描述子距离表示了两个特征之间的相似程度，不过在实际运用中还可以取不同的距离度量范数。对于浮点类型的描述子，使用欧氏距离进行度量即可。而对于二进制的描述子（比如BRIEF这样的），我们往往使用汉明距离（Hamming distance）做为度量：两个二进制串之间的汉明距离，指的是它们不同位数的个数。 

然而，当特征点数量很大时，暴力匹配法的运算量将变得很大，特别是当我们想要匹配一个帧和一张地图的时候。这不符合我们在SLAM中的实时性需求。此时快速近似最近邻（FLANN）算法更加适合于匹配点数量极多的情况。

### 2D-2D: 对极几何

#### 对极约束

现在，假设我们从两张图像中，得到了一对配对好的特征点，如下图显示的那样。如果我们有若干对这样的匹配点，就可以通过这些二维图像点的对应关系，恢复出在两帧之间摄像机的运动。这里"若干对"具体是多少对呢？我们会在下文介绍。先来看看两个图像当中的匹配点有什么几何关系吧。

以下图为例，我们希望求取两帧图像$I_1$、$I_2$之间的运动，设第一帧到第二帧的运动为$R$、$t$。两个相机中心分别为$O_1$、$O_2$。现在，考虑$I_1$中有一个特征点$p_1$，它在$I_2$中对应着特征点$p_2$。我们晓得这俩是通过特征匹配得到的。如果匹配正确，说明它们确实是同一个空间点在两个成像平面上的投影。这里我们需要一些术语来描述它们之间的几何关系。首先，连线$\overrightarrow{O_1p_1}$和连线$\overrightarrow{O_2p_2}$在三维空间中会相交于点$P$。这时候点$O_1$、$O_2$、$P$三个点可以确定一个平面，称为极平面（Epipolar plane）。$O_1O_2$连线与像平面$I_1$、$I_2$的交点分别为$e_1$、$e_2$。$e_1$、$e_2$称为极点（Epipoles），$O_1O_2$被称为基线（Baseline）。称极平面与两个像平面$I_1$、$I_2$之间的相交线$l_1$、$l_2$为极线（Epipolar line）。

![](slambook/2025-04-13-23-50-20-image.png)





## 后端

### 概述

前端视觉里程计能给出一个短时间内的轨迹和地图，但由于不可避免的误差累积，这个地图在长时间内是不准确的。所以，在视觉里程计的基础上，我们还希望构建一个尺度、规模更大的优化问题，以考虑长时间内的最优轨迹和地图。不过，考虑到精度与性能的平衡，实际当中存在着许多不同的做法。

#### 状态估计的概率解释

视觉里程计只有短暂的记忆，而我们希望整个运动轨迹在较长时间内都能保持最优的状态。我们可能会用最新的知识，更新较久远之前的状态。站在"久远的状态"的角度上看，仿佛是未来的信息告诉它"你应该在哪里"。所以，在后端优化中，我 们通常考虑一个更长时间内（或所有时间内）的状态估计问题，而且不仅使用过去的信息更新自己的状态，也会用未来的信息来更新自己，这种处理方式不妨称为"批量的"（Batch）。 否则，如果当前的状态只由过去的时刻决定，甚至只由前一个时刻决定，那不妨称为"渐进的"（Incremental）。 

我们已经知道SLAM过程可以由运动方程和观测方程来描述。那么，假设在$t=0$到$t=N$的时间内，我们有$x_0$到$x_N$那么多个位姿，并且有$y_1,...,y_M$那么多个路标。运动和观测方程为：

$\begin{cases}x_k=f(x_k,u_k)+w_k\\z_{k,j}=h(y_i,x_k)+v_{k,j}\end{cases}\quad k=1,...,N,j=1,...,M$

注意以下几点： 

1. 观测方程中，只有当$x_k$看到了$y_j$时，才会产生观测数据，否则就没有。事实上，在一个位置通常只能看到一小部分路标。而且，由于视觉SLAM特征点数量众多，所以实际当中观测方程数量会远远大于运动方程的数量。 

2. 我们可能没有测量运动的装置，所以也可能没有运动方程。在这个情况下，有若干种处理方式：认为确实没有运动方程，或假设相机不动，或假设相机匀速运动。这几种方式都是可行的。在没有运动方程的情况下，整个优化问题就只由许多个观测方程组成。这就非常类似于SfM（Structure from Motion）问题，相当于我们通过一组图像来恢复运动和结构。与SfM中不同的是，SLAM中的图像有时间上的先后顺序，而SfM中允许使用完全无关的图像。

我们知道每个方程都受噪声影响，所以要把这里的位姿$x$和路标$y$看成服从某种概率分布的随机变量，而不是单独的一个数。因此，我们关心的问题就变成了：当我拥有某些运动数据$u$和观测数据$z$时，如何来确定状态量$x$、$y$的分布？进而，如果得到了新来时刻的数据之后，那么它们的分布又将发生怎样的变化？在比较常见且合理的情况下，我们假设状态量和噪声项服从高斯分布，意味着在程序中，只需要储存它们的均值和协方差矩阵即可。均值可看作是对变量最优值的估计，而协方差矩阵则度量了它的不确定性。那么，问题转变为：当存在一些运动数据和观测数据时，我们如何去估计状态量的高斯分布？ 

我们依然设身处地地扮演一下小萝卜。只有运动方程时，相当于我们蒙着眼睛在一个未知的地方走路。尽管我们知道自己每一步走了多远，但是随着时间增长，我们将对自己的位置越来越不确定，内心也就越加不安。这说明在输入数据受噪声影响时，我们对位置方差的估计将越来越大。但是，当我们睁开眼睛时，由于能够不断地观测到外部场景，使得位置估计的不确定性变小了，我们就会越来越自信。如果用椭圆或椭球直观地表达协方差阵，那么这个过程有点像是在手机地图软件中走路的感觉。以下图为例，当没有观测数据时，这个圆会随着运动越来越大；而如果有正确观测的话，圆就会缩小至一定的大小，保持稳定。不确定性的直观描述：左侧，只有运动方程时，由于下一个时刻的位姿是在上一个时刻基础上添加了噪声，所以不确定性越来越大；右侧，存在路标点（红色）时，不确定性会明显减小。不过请注意这只是一个直观的示意图，并非实际数据。

<img src="slambook/474e6e2b211c6be1cee33b07b1e3464331f127a1.png" title="" alt="" width="440">

上面的过程以比喻的形式解释了状态估计中的问题，下面我们要以定量的方式来看待它，使用最大似然估计，把状态估计转换为最小二乘的做法。首先，由于位姿和路标点都是待估计的变量，我们改变一下记号，令$x_k$为$k$时刻的所有未知量。它包含了当前时刻的相机位姿与$m$个路标点。在这种记号的意义下，写成：

$x_k\triangleq \{x_k,y_1,...,y_m\}$

同时，把$k$时刻的所有观测记作$z_k$。于是，运动方程与观测方程的形式可写得更加简洁。这里不会出现$y$，但我们心里要明白这时$x$中已经包含了之前的$y$了：

$\begin{cases}x_k=f(x_{k-1},u_k)+w_k\\z_k=h(x_k)+v_k\end{cases}\quad k=1,...,N$

现在考虑第$k$时刻的情况。我们希望用过去$0$到$k$中的数据，来估计现在的状态分布：$P(x_k|x_0,u_{1:k},z_{1:k})$。下标$0:k$表示从$0$时刻到$k$时刻的所有数据。请注意$z_k$来表达所有在k时刻的观测数据，注意它可能不止一个，只是这种记法更加方便。

下面我们来看如何对状态进行估计。按照Bayes法则，把$z_k$与$x_k$交换位置，有：

$P(x_k|x_0,u_{11:k},z_{1:k})\propto P(z_k|x_k)P(x_k|x_0,u_{1:k},z_{1:k-1})$

这里第一项称为似然，第二项称为先验。似然由观测方程给定，而先验部分，我们要明白当前状态$x_k$是基于过去所有的状态估计得来的。至少，它会受$x_{k−1}$影响，于是按照$x_{k−1}$时刻为条件概率展开：

$P(x_k|x_0,u_{1:k},z_{1:k-1})=\int P(x_k|x_{k-1},x_0,u_{1:k},z_{1:k-1})P(x_{k-1}|x_0,u_{i:k},z_{1:k-1}){\rm d}x_{k-1}$

如果我们考虑更久之前的状态，也可以继续对此式进行展开，但现在我们只关心$k$时刻和$k−1$时刻的情况。至此，我们给出了贝叶斯估计，虽然上式还没有具体的概率分布形式，所以我还没法实际地操作它。对这一步的后续处理，方法上产生了一些分歧。大体来说，存在若干种选择：其一是假设马尔可夫性，简单的一阶马氏性认为，$k$时刻状态只与$k−1$时刻状态有关，而与再之前的无关。如果做出这样的假设，我们就会得到以扩展卡尔曼滤波（EKF）为代表的滤波器方法。在滤波方法中，我们会从某时刻的状态估计，推导到下一个时刻。另外一种方法是依然考虑k时刻状态与之前所有状态的关系，此时将得到非线性优化为主体的优化框架。目前视觉SLAM主流为非线性优化方法。

#### 线性系统

我们首先来看滤波器模型。当我们假设了马尔可夫性，从数学角度会发生哪些变化呢？ 

首先，当前时刻状态只和上一个时刻有关，以上等式右侧第一部分可进一步简化：

$P(x_k|x_{k-1},x_0,u_{1:k},z_{1:k-1})=P(x_k|x_{k-1},u_k)$

这里，由于$k$时刻状态与$k−1$之前的无关，所以就简化成只与$x_{k−1}$和$u_k$有关的形式，与$k$时刻的运动方程对应。第二部分可简化为：

$P(x_{k-1}|x_0,u_{1:k},z_{1:k-1})=P(x_{k-1}|x_0,u_{1:k-1},z_{1:k-1})$

这是考虑到$k$时刻的输入量$u_k$与$k−1$时刻的状态无关，所以我们把$u_k$拿掉。可以看到，这一项实际是$k−1$时刻的状态分布。于是，这一系列方程说明了，我们实际在做的是"如何把$k−1$时刻的状态分布推导至k时刻"这样一件事。也就是说，在程序运行期间，我们只要维护一个状态量，对它进行不断地迭代和更新即可。进一步，如果假设状态量服从高斯分布，那我们只需考虑维护状态量的均值和协方差即可。 

我们从形式最简单的线性高斯系统开始，最后会得到卡尔曼滤波器。线性高斯系统是说，运动方程和观测方程可以由线性方程来描述：

$\begin{cases}x_k=A_kx_{k-1}+u_k+w_k\\z_k=C_kx_k+v_k\end{cases}\quad k=1,...,N$

并假设所有的状态和噪声均满足高斯分布。记这里的噪声服从零均值高斯分布：

$w_k\sim N(0,R)\quad v_k\sim N(0,Q)$

为了简洁，将R和Q的下标省略。现在，利用马尔可夫性，假设我们知道了$k−1$时刻的后验（在$k−1$时刻看来）状态估计：$\hat{x}_{k-1}$和它的协方差$\hat{P}_{k-1}$，现在要根据$k$时刻的输入和观测数据，确定$x_k$的后验分布。为区分推导中的先验和后验，我们在记号上作一点区别：以尖帽子$\hat{x}_{k}$表示后验，以横线$\overline{x}$表示先验分布。 

卡尔曼滤波器的第一步，通过运动方程确定x_k的先验分布。根据高斯分布的性质，显然有：

$P(x_k|x_0,u_{1:k},z_{1:k-1})=N(A_k\hat{x}_{k-1}+u_k,A_k\hat{P}_{k-1}A^T_k+R)$

这一步称为预测。它显示了如何从上一个时刻的状态，根据输入信息（但是有噪声），推断当前时刻的状态分布。这个分布也就是先验。记这里的：

$\hat{x}_k=A_k\hat{x}_{k-1}+u_k,\quad \overline{P}_k=A_k\hat{P}_{k-1}A^T_k+R$

这非常自然。另一方面，由观测方程，我们可以计算在某个状态下，应该产生怎样的观测数据：

$P(z_k|x_k)=N(C_kx_k,Q)$

为了得到后验概率，我们想要计算它们的乘积，也就是由贝叶斯公式。然而，虽然我们知道最后会得到一个关于$x_k$的高斯分布，但计算上是有点麻烦的，我们先把结果设为：

$x_k\sim N(C_kx_k,Q)$

那么：

$N(\hat{x}_k,\hat{P}_k)=N(C_kx_k,Q)\cdot N(\overline{x}_k,\overline{P}_k)$

既然我们已经知道等式两侧都是高斯分布，那就只需比较指数部分即可，而无须理会高斯分布前面的因子部分。指数部分很像是一个二次型的配方，我们来推导一下。首先把指数部分展开，有：

$(x_k-\hat{x}_k)^T\hat{P}^{-1}_k(x_k-\hat{x}_k)=(z_k-C_kx_k)^TQ^{-1}(z_k-C_kx_k)+(x_k-\overline{x}_k)^T\overline{P}^{-1}_k(x_k-\overline{x}_k)$

为了求左侧的$\hat{x}_k$和$\hat{P}_k$，我们把两边展开，并比较$x_k$的二次和一次系数。对于二次系数，有：

$\hat{P}^{-1}_k=C^T_kQ^{-1}C_k+\hat{P}^{-1}_k$

该式给出了协方差的计算过程。为了便于后边列写式子，定义一个中间变量：

$K=\hat{P}_kC^T_kQ^{-1}$

根据此定义，在上式左右各乘$\hat{P}_k$，有：

$I=\hat{P}_kC^{T}_kQ^{-1}C_k+\hat{P}_k\overline{P}^{-1}_k=KC_k+\hat{P}_k\overline{P}^{-1}_k$

于是有：

$\hat{P}_k=(I-KC_k)\overline{P}_k$

然后再比较一次项的系数，有：

$-2\hat{x}^T_k\hat{P}^{-1}_kx_k=-2z^T_kQ^{-1}C_kx_k-2\overline{x}^T_k\overline{P}^{-1}_kx_k$

整理（取系数并转置）得：

$\hat{P}^{-1}_k\hat{x}_k=C^T_kQ^{-1}z_k+\overline{P}^{-1}_k\overline{x}_k$

两侧乘以$\hat{P}_k$，得：

$\hat{x}_k=\hat{P}_kC^T_kQ^{-1}z_k+\hat{P}_k\overline{P}^{-1}_k\overline{x}_k=Kz_k+(I-KC_k)\overline{x}_k=\overline{x}_k+K(z_k-C_k\overline{x}_k)$

于是我们又得到了后验均值的表达。总而言之，上面的两个步骤可以归纳为"预测"（Predict）和"更新"（Update）两个步骤：

1. 预测：$\hat{x}_k=A_k\hat{x}_{k-1}+u_k,\quad \overline{P}_k=A_k\hat{P}_{k-1}A^T_k+R$

2. 更新：先计算$K$，它又称为卡尔曼增益：$K=\overline{P}_kC^T_k(C_k\overline{P}_kC^T_k+Q)^{-1}$
   
   然后计算后验概率的分布：$\hat{x}_k=\overline{x}_k+K(z_k-C_k\overline{x}_k),\quad \hat{P}_k=(I-KC_k)\overline{P}_k$

至此，我们推导了经典的卡尔曼滤波器的整个过程。事实上卡尔曼滤波器有若干种推导方式，而我们使用的是从概率角度出发的最大后验概率估计的形式。我们看到，在线性高斯系统中，卡尔曼滤波器构成了该系统中的最大后验概率估计。而且，由于高斯分布经过线性变换后仍服从高斯分布，所以整个过程中我们没有进行任何的近似。可以说，卡尔曼滤波器构成了线性系统的最优无偏估计。

#### 非线性系统和EKF

在理解卡尔曼滤波之后，我们必须要澄清一点：SLAM中的运动方程和观测方程通常是非线性函数，尤其是视觉SLAM中的相机模型，需要使用相机内参模型以及李代数表示的位姿，更不可能是一个线性系统。一个高斯分布，经过非线性变换后，往往不再是高斯分布，所以在非线性系统中，我们必须取一定的近似，将一个非高斯的分布近似成一个高斯分布。 

我们希望把卡尔曼滤波器的结果拓展到非线性系统中来，称为扩展卡尔曼滤波器（Extended Kalman Filter，EKF）。通常的做法是，在某个点附近考虑运动方程以及观测方程的一阶泰勒展开，只保留一阶项，即线性的部分，然后按照线性系统进行推导。令k−1时刻的均值与协方差矩阵为$\hat{x}_{k-1}$、\$hat{P}{k-1}$。在$k时刻，我们把运动方程和观测方程，在$\hat{x}_{k-1}$、$\hat{P}_{k-1}$处进行线性化（相当于一阶泰勒展开），有：

$x_k\approx f(\hat{x}_{k-1},u_k)+\frac{\partial f}{\partial x_{k-1}}\big|_{\hat{x}_{k-1}}(x_{k-1}-\hat{x}_{k-1})+w_k$

记这里的偏导数为：

$F=\frac{\partial f}{\partial x_{k-1}}\big|_{\hat{x}_{k-1}}$

同样的，对于观测方程，亦有：

$z_k\approx h(\overline{x}_k)+\frac{\partial h}{\partial x_k}\big|_{\hat{x}_k}(x_k-\hat{x}_k)+n_k$

记这里的偏导数为：

$H=\frac{\partial h}{\partial x_k}\big|_{\overline{x}_k}$

那么，在预测步骤中，根据运动方程有：

$P(x_k|x_0,u_{1:k},z_{0:k-1})=N(f(\hat{x}_{k-1},u_k),F\hat{P}_{k-1}F^T+R_k)$

这些推导和卡尔曼滤波是十分相似的。为方便表述，记这里先验和协方差的均值为：

$\overline{x}_k=f(\hat{x}_{k-1},u_k),\quad \overline{P}_k=F\hat{P}_kF^T+R_k$

然后，考虑在观测中，我们有：

$P(z_k|x_k)=N(h(\overline{x}_k)+H(x_k-\overline{x}_k),Q_k)$

最后，根据最开始的Bayes展开式，可以推导出$x_k$的后验概率形式。我们略去中间的推导过程，只介绍其结果。读者可以仿照着卡尔曼滤波器的方式，推导EKF的预测与更新方程。简而言之，我们会先定义一个卡尔曼增益$K_k$：

$K_k=\overline{P}_kH^T(H\hat{P}_kH^T+Q_k)^{-1}$

在卡尔曼增益的基础上，后验概率的形式为：

$\hat{x}_k=\overline{x}_k+K_k(z_k-h(\overline{x}_k)),\quad \hat{P}_k=(I-K_kH)\overline{P}_k$

卡尔曼滤波器给出了在线性化之后，状态变量分布的变化过程。在线性系统和高斯噪声下，卡尔曼滤波器给出了无偏最优估计。而在SLAM这种非线性的情况下，它给出了单次线性近似下最大后验估计（MAP）。

#### EKF的讨论

EKF以形式简洁、应用广泛著称。当我们想要在某段时间内估计某个不确定量时，首先想到的就是EKF。在早期的SLAM中，EKF占据了很长一段时间的主导地位，研究者们讨论了各种各样滤波器在SLAM中的应用，如IF（信息滤波器）、IEKF（Iterated KF）、UKF（Unscented KF）、粒子滤波器、SWF(Sliding Window Filter）等等，或者用分治法等思路改进EKF的效率。直至今日，尽管我们认识到非线性优化比滤波器占有明显的优势，但是在计算资源受限，或待估计量比较简单的场合，EKF仍不失为一种有效的方式。 

EKF有哪些局限呢？ 

1. 首先，滤波器方法在一定程度上假设了马尔可夫性，也就是k时刻的状态只与k−1时刻相关，而与k−1之前的状态和观测都无关（或者和前几个有限时间的状态相关）。这有点像是在视觉里程计中，只考虑相邻两帧关系一样。如果当前帧确实与很久之前的数据有关（例如回环），那么滤波器就会难以处理这种情况。 
   
   而非线性优化方法则倾向于使用所有的历史数据。它不光考虑邻近时刻的特征点与轨迹关系，更会把考虑很久之前的状态也考虑进来，称为全体时间上的SLAM（Full-SLAM）。在这种意义下，非线性优化方法使用了更多信息，当然也需要更多的计算。 

2. 与其它优化方法相比，EKF滤波器仅在\hat{x}_{k−1}处做了一次线性化，然后就直接根据这次线性化结果，把后验概率给算了出来。这相当于在说，我们认为该点处的线性化近似，在后验概率处仍然是有效的。而实际上，当我们离开工作点较远的时 候，一阶泰勒展开并不一定能够近似整个函数，这取决于运动模型和观测模型的非线 性情况。如果它们有强烈的非线性，那线性近似就只在很小范围内成立，不能认为在 很远的地方仍能用线性来近似。这就是 EKF 的非线性误差，是它的主要问题所在。 

3. 在优化问题中，尽管我们也做一阶（最速下降）或二阶（G-N或L-M）的近似，但每迭代一次，状态估计发生改变之后，我们会重新对新的估计点做泰勒展开，而不像EKF那样只在固定点上做一次泰勒展开。这就导致优化方法适用范围更广，则在状 态变化较大时亦能适用。 

从程序实现上来说，EKF需要存储状态量的均值和方差，并对它们进行维护和更新。如果把路标也放进状态的话，由于视觉SLAM中路标数量很大，这个存储量是相当可观的，且与状态量呈平方增长（因为要存储协方差矩阵）。因此，EKF SLAM普遍被认为不可适用于大型场景。 

由于EKF存在这些明显的缺点，我们通常认为，在同等计算量的情况下，非线性优化能取得更好的效果。

### BA与图优化

#### 投影模型和BA代价函数

从一个世界坐标系中的点$p$出发，把相机的内外参数和畸变都考虑进来，最后投影成像素坐标，一共需要如下几个步骤： 

1. 首先，把世界坐标转换到相机坐标，这里将用到相机外参数$(R,t)$：
   
   $P'=Rp+t=[X',Y',Z']^T$

2. 然后，将$P'$投至归一化平面，得到归一化坐标：
   
   $P_c=[u_c,v_c,1]^T=[X'/Z',Y'/Z',1]$

3. 对归一化坐标去畸变，得到去畸变后的坐标。这里暂时只考虑径向畸变：
   
   $\begin{cases}u'_c=u_c(1+k_1r^2_c+k_2r^4_c)\\v'_c=v_c(1+k_1r^2_c+k_2r^4_c)\end{cases}$

这一系列计算流程看似有些复杂。我们用流程图形象化地表示整个过程：

<img title="" src="slambook/2025-04-07-16-13-17-GetImage.png" alt="" width="589">

左侧的$p$是全局坐标系下的三维坐标点，右侧的$u_s$、$v_s$是该点在图像平面上的最终像素坐标。中间畸变模块中的$r_c^2 = u_c^2 + v_c^2$。

这个过程也就是观测方程，之前我们把它抽象地记成：

$z=h(x,y)$

现在，我们给出了它的详细参数化过程。具体地说，这里的$x$指代此时相机的位姿，即外参$R$、$t$，它对应的李代数为$\xi$。路标$y$即这里的三维点$p$，而观测数据则是像素坐标：

$z\triangleq [u_s,v_s]^T$

以最小二乘的角度来考虑，那么可以列写关于此次观测的误差：

$e=z-h(\xi,p)$

然后，把其它时刻的观测量也考虑进来，我们可以给误差添加一个下标。设$z_{ij}$为在位姿$\xi_i$处观察路标$p_j$产生的数据，那么整体的代价函数（Cost Function）为：

$\frac{1}{2}\sum\limits^m_{i=1}\sum\limits^m_{j=1}\Vert e_{ij}\Vert^2=\frac{1}{2}\sum\limits^m_{i=1}\sum\limits^m_{j=1}\Vert z_{ij}-h(\xi_i,p_j)\Vert^2$

对这个最小二乘进行求解，相当于对位姿和路标同时作了调整，也就是所谓的BA。

#### BA的求解

观察观测模型$h$，很容易判断该函数不是线性函数。所以我们希望一些非线性优化手段来优化它。根据非线性优化的思想，我们应该从某个的初始值开始，不断地寻找下降方向$\Delta{x}$来找到目标函数的最优解。尽管误差项都是针对单个位姿和路标点的，但在整体BA目标函数上，我们必须把自变量定义成所有待优化的变量：

$x=[\xi_1,...,\xi_m,p_1,..,p_n]^T$

相应的，增量方程中的$\Delta{x}$则是对整体自变量的增量。在这个意义下，当我们给自变量一个增量时，目标函数变为：

$\frac{1}{2}\Vert f(x+\Delta x)\Vert^2\approx\frac{1}{2}\Vert e_{ij}+F_{ij}\Delta\xi_i+E_{ij}\Delta p_j\Vert^2$

其中$F_{ij}$表示整个代价函数在当前状态下对相机姿态的偏导数，而$E_{ij}$表示该函数对路标点位置的偏导。现在，把相机位姿变量放在一起：

$x_c=[\xi_1,\xi_2,...,\xi_m]^T\in\mathbb{R}$

并把空间点的变量也放在一起：

那么，上式可以简化表达为如下：

$\frac{1}{2}\Vert f(x+\Delta x)\Vert^2=\frac{1}{2}\Vert e+F\Delta x_c+E\Delta x_p\Vert^2$

需要注意的是，该式从一个由很多个小型二次项之和，变成了一个更整体的样子。这里的雅可比矩阵$E$和$F$必须是整体目标函数对整体变量的导数，它将是一个很大块的矩阵，而里头每个小分块，需要由每个误差项的导数$F_{ij}$和$E_{ij}$拼凑起来。然后，无论我们使用G-N还是L-M方法，最后都将面对增量线性方程：

$H\Delta x=g$

G-N和L-M的主要差别在于，这里的$H$是取$J^TJ$还是$J^TJ+\lambda{I}$的形式。由于我们把变量归类成了位姿和空间点两种，所以雅可比矩阵可以分块为：

$J=[F E]$

那么，以G-N为例，则$H$矩阵为：

$H=J^TJ=\begin{bmatrix} F^TF&F^TE\\E^TF&E^TE\end{bmatrix}$

当然在L-M中我们也需要计算这个矩阵。不难发现，因为考虑了所有的优化变量，这个线性方程的维度将非常大，包含了所有的相机位姿和路标点。尤其是在视觉SLAM中，一个图像就会提出数百个特征点，大大增加了这个线性方程的规模。如果直接对$H$求逆来计算增量方程，由于矩阵求逆是复杂度为$O(n^3)$的操作，这是非常消耗计算资源的。 

幸运地是，这里的$H$矩阵是有一定的特殊结构的。利用这个特殊结构，我们可以加速求解过程。

#### 稀疏性和边缘化

21世纪视觉SLAM的一个重要进展是认识到了矩阵$H$的稀疏结构，并发现该结构可以自然、显式地用图优化来表示。 

$H$矩阵的稀疏性是由雅可比$J(x)$引起的。考虑这些代价函数当中的其中一个$e_{ij}$。注意到，这个误差项只描述了在$\xi_i$看到$p_j$这件事，只涉及到第$i$个相机位姿和第$j$个路标点，对其余部分的变量的导数都为$0$。所以该误差项对应的雅可比矩阵有下面的形式：

$J_{ij}(x)=(0_{2\times 6},...,0_{2\times 6},\frac{\partial e_{ij}}{\partial\xi_i},0_{2\times 3},...,0_{2\times 3},\frac{\partial e_{ij}}{\partial p_j},0_{2\times 3},...,0_{2\times 3})$

其中$0_{2\times6}$表示维度为2×6的0矩阵，同理$0_{2\times3}$也是一样。该误差项对相机姿态的偏导$\frac{\partial{e_{ij}}}{\partial{\xi_{i}}}$的维度为$2\times6$，对路标点的偏导$\frac{\partial{e_{ij}}}{\partial{p_j}}$维度是$2\times3$。这个误差项的雅可比矩阵，除了这两处为非零块之外，其余地方都为零。这体现了该误差项与其它路标和轨迹无关的特性。那么，它对增量方程有何影响呢？$H$矩阵为什么会产生稀疏性呢？

<img src="slambook/2025-04-07-17-58-58-GetImage(1).png" title="" alt="" width="428">

我们设$J_{ij}$只在$(i,j)$处有非零块，那么它对$H$的贡献为$J^T_{ij}J_{ij}$，具有上图所画的稀疏形式。这个$J^T_{ij}J_{ij}$矩阵也仅有四个非零块，位于$(i,i)$、$(i,j)$、$(j,i)$、$(j,j)$。对于整体的$H$，由于：

请注意$i$在所有相机位姿中取值，$j$在所有路标点中取值。我们把$H$进行分块：

$H=\begin{bmatrix}H_{11}&H_{12}\\H_{21}&H_{22}\end{bmatrix}$

这里$H_{11}$只和相机位姿有关，而$H_{22}$只和路标点有关。当我们遍历$(i,j)$时，以下事实总是成立的：

1. 不管$(i,j)$怎么变，$H_{11}$都是对角阵，只在$H(i,i)$处有非零块； 

2. 同理，$H_{22}$也是对角阵，只在$(j,j)$处有非零块； 

3. 对于$H_{12}$和$H_{21}$，它们可能是稀疏的，也可能是稠密的，视具体的观测数据而定。

这显示了$H$的稀疏结构。之后对线性方程的求解中，也正需要利用它的稀疏结构。也许读者还没有很好地领会这里的意思，我们举一个实例来直观说明它的情况。假设一个场景内有2个相机位姿$(C_1,C_2)$和6个路标$(P_1,P_2,P_3,P_4,P_5,P_6)$。这些相机和点云所对应的变量为$\xi_i\ (i=1,2)$以及$p_j\ (j=1,...,6)$。相机$C_1$观测到路标$(P_1,P_2,P_3,P_4)$，相机$C_2$观测到了路标$(P_3,P_4,P_5,P_6)$。我们把这个过程画成如下示意图。相机和路标以圆形节点表示。如果$i$相机能够观测到$j$点云，我们就在它们对应的节点连上一条边。

<img title="" src="slambook/2025-04-08-11-54-00-GetImage(2).png" alt="" width="336">

可以推出该场景下的BA目标函数应该是：

$\frac{1}{2}(\Vert e_{11}\Vert^2+\Vert e_{12}\Vert^2+\Vert e_{13}\Vert^2+\Vert e_{14}\Vert^2+\Vert e_{23}\Vert^2+\Vert e_{24}\Vert^2+\Vert e_{25}\Vert^2+\Vert e_{26}\Vert^2)$

以$e_{11}$为例，它描述了在$C_1$看到了$P_1$这件事，与其它的相机位姿和路标无关。令$J_{11}$为$e_{11}$所对应的雅可比矩阵，不难看出$e_{11}$对相机变量$\xi_2$和路标点$p_2,...,p_6$的偏导都为$0$。我们把所有变量以$x=(\xi_1,\xi_2,p_1,...,p_2)^T$的顺序摆放，则有：

为了方便表示稀疏性，我们用带有颜色的方块表示矩阵在该方块内有数值，其余没有颜色的区域表示矩阵在该处数值都为$0$。那么上面的$J_{11}$则可以表示成如下图案。同理，其它的雅可比矩阵也会有类似的稀疏图案。

<img title="" src="slambook/2025-04-08-14-25-46-GetImage(3).png" alt="" width="283">

上图为$J_{11}$矩阵的非零块分布图。上方的标记表示矩阵该列所对应的变量。由于相机参数维数比点云参数维数要大，所以$C_1$对应的矩阵块要比$P_1$对应的矩阵块要宽一些。 

为了得到该目标函数对应的雅可比矩阵，我们可以将这些$J_{ij}$按照一定顺序列为向量，那么整体雅可比矩阵以及相应的$H$矩阵的稀疏情况就是下图中所展示的那样。

<img title="" src="slambook/2025-04-08-14-27-53-GetImage(4).png" alt="" width="542">

Jacobian矩阵的稀疏性（左）和H矩阵的稀疏性（右），蓝色的方块表示矩阵在对应的矩阵块处有数值，其余没有颜色的部分表示矩阵在该处的数值始终为$0$。 

邻接矩阵（Adjacency Matrix，所谓邻接矩阵是这样一种矩阵，它的第$(i,j)$个元素描述了节点$i$和$j$是否存在一条边，如果存在此边，设这个元素为$1$，否则设为$0$。相机$i$与点云$j$构成的邻接矩阵和$H$矩阵，除了对角元素以外的其余部分有着完全一致的结构。事实上的确如此。$H$矩阵一共有$8\times 8$个矩阵块，对于H矩阵当中处于非对角线的矩阵块来说，如果该矩阵块非零，则其位置所对应的变量之间会在图中存在一条边，我们可以从下图中清晰地看到这一点。所以，$H$矩阵当中的非对角部分的非零矩阵块可以理解为它对应的两个变量之间存在联系，或者可以称之为约束。于是，我们发现图优化结构与增量方程的稀疏性存在着明显的联系。

<img src="slambook/2025-04-08-14-43-54-GetImage(5).png" title="" alt="" width="565">

如上右图$H$矩阵中非零矩阵块和左图中边的对应关系。如左图当中的$H$矩阵当中红色的矩阵块，表示在右图中其对应的变量$C_2$和$P_6$之间存在一条边$e_{26}$。 

现在考虑更一般的情况，假如我们有$m$个相机位姿，$n$个路标点。由于通常路标数量远远会比相机多，于是有$n$远大于$m$。由上面推理可知，实际当中的$H$矩阵会像下图所示的那样。它的左上角块显得非常小，而右下角的对角块占据了大量地方。除此之外，非对角部分则分布着散乱的观测数据。由于它的形状很像箭头，又称为箭头形（Arrow-like）矩阵。同时它又很像一把镐子，所以也叫镐形矩阵。 

对于具有这种稀疏结构的$H$，线性方程$H\Delta{x}=g$的求解会有什么不同呢？现实当中存在着若干种利用$H$的稀疏性加速计算的方法，而本节介绍视觉SLAM里一种最常用的手段：Schur消元（Schur Trick）。在SLAM研究中亦称为Marginalization（边缘化）。 

仔细观察下图，我们不难发现这个矩阵可以分成四个块。左上角为对角块矩阵，每个对角块元素的维度与相机位姿的维度相等，且是一个对角块矩阵。右下角也是对角块矩阵，每个对角块的维度是路标的维度。非对角块的结构与具体观测数据相关。我们首先将这个矩阵划分为四个区域。为了后续分析地方便，我们称这四个块为$B$、$E$、$C$。

<img src="slambook/2025-04-08-14-46-22-GetImage(6).png" title="" alt="" width="484">

于是，对应的线性方程组也可以由$H\Delta{x}=g$变为如下形式：

$\begin{bmatrix}B&E\\E^T&C\end{bmatrix}\begin{bmatrix}\Delta x_c\\\Delta x_p\end{bmatrix}=\begin{bmatrix}v\\w\end{bmatrix}$

其中B是对角块矩阵，每个对角块的维度和相机参数的维度相同，对角块的个数是相机变量的个数。由于路标数量会远远大于相机变量个数，所以$C$往往也远大于**B**。三维空间中每个路标点为三维，于是$C$矩阵为对角块矩阵，每个块为$3\times3$维矩阵。对角块矩阵逆的难度远小于对一般矩阵的求逆难度，因为我们只需要对那些对角线矩阵块分别求逆即可。考虑到这个特性，我们线性方程组进行高斯消元，目标是消去右上角的非对角部分$E$，得：

$\begin{bmatrix}I&-EC^{-1}\\0&I\end{bmatrix}\begin{bmatrix}b&e\\e^T&c\end{bmatrix}\begin{bmatrix}\Delta x_c\\\Delta x_p\end{bmatrix}=\begin{bmatrix}I&-EC^{-1}\\0&I\end{bmatrix}\begin{bmatrix}v\\w\end{bmatrix}$

整理得：

$\begin{bmatrix}B-EC^{-1}E^T&0\\E^T&C\end{bmatrix}\begin{bmatrix}\Delta x_c\\\Delta x_p\end{bmatrix}=\begin{bmatrix}v-EC^{-1}w\\w\end{bmatrix}$

经过消元之后，第一行方程组变成和$\Delta{x}_p$无关的项。单独把它拿出来，得到关于位姿部分的增量方程：

$\begin{bmatrix}B-EC^{-1}E^T\end{bmatrix}\Delta x_c=v-EC^{-1}w$

这个线性方程组的维度和B矩阵一样。我们的做法是先求解这个方程，然后把解得的$\Delta{x}_c$代入到原方程，然后求解$\Delta{x}_p$。这个过程称为Marginalization，或者Schur消元（Schur Elimination）。相比于直接解线性方程的做法，它的优势在于：

1. 在消元过程中，由于$C$为对角块，所以$C^{−1}$容易解得。 

2. 求解了$\Delta{x}_c$之后，路标部分的增量方程由$\Delta{x}_p=C^{−1}(\omega−E^T\Delta{x}_c)$给出。这依然用到了$C^{−1}$易于求解的特性。

关于这个方程，它仅是一个普通的线性方程，没有特殊的结构可以利用。我们将此方程的系数记为$S$，它的稀疏性如何呢？下图显示了进行Schur消元之后的一个$S$实例，可以看到它的稀疏性是不规则的。 

前面说到，H矩阵的非对角块处的非零元素对应着相机和路标的关联。那么，进行了Schur消元后S的稀疏性是否具有物理意义呢？答案是有的。此处我们不加以证明地说，$S$矩阵的非对角线上的非零矩阵块，表示了该处对应的两个相机变量之间存在着共同观测的路标点，有时候称为共视（Co-visibility）。反之，如果该块为零，则表示这两个相机没有共同观测。

<img src="slambook/2025-04-09-10-30-15-GetImage(7).png" title="" alt="" width="245">

例如下图所示的稀疏矩阵，左上角前4×4个矩阵块可以表示对应的相机变量$C_1$、$C_2$、$C_3$、$C_4$之间有共同观测。

<img src="slambook/2025-04-09-10-31-24-GetImage(8).png" title="" alt="" width="400">

于是，S矩阵的稀疏性结构当取决于实际观测的结果，我们无法提前预知。在实践当中，例如ORB_SLAM中的Local Mapping环节，在做BA的时候刻意选择那些具有共同观测的帧作为关键帧，在这种情况下Schur消元后得到的S就是稠密矩阵。不过，由于这个模块并不是实时执行，所以这种做法也是可以接受的。但是在另一些方法里面，例如DSO、OKVIS等，它们采用了滑动窗口方法（Sliding Window）。这类方法对每一帧都要求做一次BA来防止误差的累积，因此它们也必须采用一些技巧来保持$S$矩阵的稀疏性。 

从概率角度来看，我们称这一步为边缘化，是因为我们实际上把求$(\Delta{x}_c,\Delta{x}_p)$的问题，转化成先求$\Delta{x}_c$，再求$\Delta{x}_p$的过程。这一步相当于做了条件概率展开：

$P(x_c,x_p)=P(x_c)\cdot P(x_p|x_c)$

结果是求出了关于$x_c$的边缘分布，故称边缘化。在上边讲的边缘化过程中，我们实际把所有的路标点都给边缘化了。根据实际情况，我们也能选择一部分进行边缘化。同时，Schur消元只是实现边缘化的其中一种方式，同样可以使用Cholesky分解进行边缘化。

#### 鲁棒核函数

在前面的BA问题中，我们最小化误差项的二范数平方和，作为目标函数。这种做法虽然很直观，但存在一个严重的问题：如果出于误匹配等原因，某个误差项给的数据是错误的，会发生什么呢？我们把一条原本不应该加到图中的边给加进去了，然而优化算法并不能辨别出这是个错误数据，它会把所有的数据都当作误差来处理。这时，算法会看到一条误差很大的边，它的梯度也很大，意味着调整与它相关的变量会使目标函数下降更多。所以，算法将试图调整这条边所连接的节点的估计值，使它们顺应这条边的无理要求。由于这个边的误差真的很大，往往会抹平了其他正确边的影响，使优化算法专注于调整一个错误的值。这显然不是我们希望看到的。 

出现这种问题的原因是，当误差很大时，二范数增长得太快了。于是就有了核函数的存在。核函数保证每条边的误差不会大的没边，掩盖掉其他的边。具体的方式是，把原先误差的二范数度量，替换成一个增长没有那么快的函数，同时保证自己的光滑性质（不然没法求导）。因为它们使得整个优化结果更为鲁棒，所以又叫它们为鲁棒核函数（Robust Kernel）。 

鲁棒核函数有许多种，例如最常用的Huber核：

$H(e) = \begin{cases}\frac{1}{2}e^2&\text{if }|e|\leq\delta,\\\delta(|e|-\frac{1}{2}\delta)& \text{otherwise.}\end{cases}$

我们看到，当误差$e$大于某个阈值$\delta$后，函数增长由二次形式变成了一次形式，相当于限制了梯度的最大值。同时，Huber核函数又是光滑的，可以很方便地求导。下图显示了Huber核函数与二次函数的对比，可见在误差较大时Huber核函数增长明显低于二次函数。

<img src="slambook/2025-04-09-11-58-38-GetImage(9).png" title="" alt="" width="325">

除了Huber核之外，还有Cauchy核、Tukey核等等。

#### 小结

在实践当中，有许多软件库为我们提供了细节操作，而我们需要做的主要是构造Bundle Adjustment问题，设置Schur消元，然后调用稠密或者稀疏矩阵求解器对变量进行优化即可。

### 后端位姿图

#### Pose Graph的意义

带有相机位姿和空间点的图优化称为BA，能够有效地求解大规模的定位与建图问题。但是，随着时间的流逝，机器人的运动轨迹将越来越长，地图规模也将不断增长。像BA这样的方法，计算效率就会（令人担忧地）不断下降。根据前面的讨论，我们发现特征点在优化问题中占据了绝大多数部分。而实际上，经过若干次观测之后，那些收敛的特征点，空间位置估计就会收敛至一个值保持不动，而发散的外点则通常看不到了。对收敛点再进行优化，似乎是有些费力不讨好的。因此，我们更倾向于在优化几次之后就把特征点固定住，只把它们看作位姿估计的约束，而不再实际地优化它们的位置估计。 

沿着这个思路往下走，我们会发现：是否能够完全不管路标，而只管轨迹呢?我们完全可以构建一个只有轨迹的图优化，而位姿节点之间的边，可以由两个关键帧之间通过特征匹配之后得到的运动估计来给定初始值。不同的是，一旦初始估计完成，我们就不再优化那些路标点的位置，而只关心所有的相机位姿之间的联系了。通过这种方式，我们省去了大量的特征点优化的计算，只保留了关键帧的轨迹，从而构建了所谓的位姿图（Pose Graph），如下图所示。

<img src="slambook/2025-04-09-13-26-59-GetImage(10).png" title="" alt="" width="422">

我们知道在BA中，特征点数量远大于位姿节点的数量。一个关键帧往往关联了数百个关键点，而实时BA的最大计算规模，即使利用稀疏性，在当前的主流CPU上一般也就是几万个点左右。这就限制了SLAM应用场景。所以，当机器人在更长的时间和空间中运动时，必须考虑一些解决方式：要么像滑动窗口法那样，丢弃一些历史数据；要么像Pose Graph的做法那样，舍弃对路标点的优化，只保留Pose之间的边，使用Pose Graph。

#### Pose Graph的优化

那么，Pose Graph图优化中的节点和边都是什么意思呢？这里的节点表示相机位姿，以$\xi_1,...,\xi_n$来表达。而边，则是两个位姿节点之间相对运动的估计，该估计可能来自于特征点法或直接法，但不管如何，我们估计了，比如说$\xi_i$和$\xi_j$之间的一个运动$\Delta\xi_{ij}$。该运动可以有若干种表达方式，我们取比较自然的一种：

$\Delta\xi_{ij}=\xi^{-1}_i\xi_j=\ln(\exp((-\xi_i)^{\wedge})\exp(\xi^{\wedge}_j))^{\vee}$

或按李群的写法：

$\Delta T_{ij}=T^{-1}_iT_j$

按照图优化的思路来看，实际当中该等式不会精确地成立，因此我们设立最小二乘误差，然后和以往一样，讨论误差关于优化变量的导数。这里，我们把上式的$\Delta{T_{ij}}$移至等式右侧，构建误差$e_{ij}$：

$e_{ij}=\ln(T^{-1}_{ij}T^{-1}_iT_j)^{\vee}=\ln(\exp((-\xi_{ij})^{\wedge})\exp((-\xi_i)^{\wedge})\exp(\xi^{\wedge}_j))^{\vee}$

注意优化变量有两个：$\xi_i$和$\xi_j$，因此我们求$e_{ij}$关于这两个变量的导数。按照李代数的求导方式，给$\xi_i$和$\xi_j$各一个左扰动$\delta\xi_i$和$\delta\xi_j$。于是误差变为：

$\hat{e}_{ij}=\ln(T^{-1}_{ij}T^{-1}_i\exp((-\delta\xi_i)^{\wedge})\exp(\delta\xi^{\wedge}_j)T_j)^{\vee}$

该式中，两个扰动项被夹在了中间。为了利用BCH近似，我们希望把扰动项移至式子左侧或右侧：

$\exp(({\rm Ad}(T)\xi)^{\wedge})=T\exp(\xi^{\wedge})T^{-1}$

该式表明，通过引入一个伴随项，我们能够"交换"扰动项左右侧的T。利用它，可以将扰动挪到最右（当然最左亦可），导出右乘形式的雅可比矩阵（挪到左边时形成左乘）：

$\hat{e}_{ij}=\ln(T^{-1}_{ij}T^{-1}_i\exp((-\delta\xi_i)^{\wedge})\exp(\delta\xi^{\wedge}_j)T_j)^{\vee}$

该式中，两个扰动项被夹在了中间。为了利用BCH近似，我们希望把扰动项移至式子左侧或右侧：

$\exp(({\rm Ad}(T)\xi)^{\wedge})=T\exp(\xi^{\wedge})T^{-1}$

该式表明，通过引入一个伴随项，我们能够"交换"扰动项左右侧的$T$。利用它，可以将扰动挪到最右（当然最左亦可），导出右乘形式的雅可比矩阵（挪到左边时形成左乘）：

$\begin{aligned}\hat{e}_{ij}&=\ln(T^{-1}_{ij}T^{-1}_i\exp((-\delta\xi_i)^{\wedge})\exp(\delta\xi^{\wedge}_j)T_j)^{\vee}\\&=\ln(T^{-1}_{ij}T^{-1}_{i}T_j\exp((-{\rm Ad(T^{-1}_j)\delta\xi_i})^{\wedge})\exp(({\rm Ad}(T^{-1}_j)\delta\xi_j)^{\wedge})^{\vee}\\&\approx\ln(T^{-1}_{ij}T^{-1}_iT_j[I-({\rm Ad}(T^{-1}_j)\delta\xi_i)^{\wedge}+({\rm Ad}(T^{-1}_j)\delta\xi_j)^{\wedge}])^{\vee}\\&\approx e_{ij}+\frac{\partial e_{ij}}{\partial \delta\xi_i}\delta\xi_i+\frac{\partial e_{ij}}{\partial\delta\xi_j}\delta\xi_j\end{aligned}$

因此，按照李代数上的求导法则，我们求出了误差关于两个位姿的雅可比矩阵。关于$T_i$的：

$\frac{\partial e_{ij}}{\partial\delta\xi_{i}}=-\mathcal{J}^{-1}_r(e_{ij}){\rm Ad(T^{-1}_j)}$

以及关于$T_j$的：

$\frac{\partial e_{ij}}{\partial\delta\xi_{j}}=-\mathcal{J}^{-1}*r(e*{ij}){\rm Ad(T^{-1}_j)}$

由于$\mathfrak{se}(3)$上的左右雅可比$\mathcal{J}_r$形式过于复杂，我们通常取它们的近似。如果误差接近于零，我们就可以设它们近似为$I$或：

$\mathcal{J}^{-1}_r(e_{ij})\approx I+\frac{1}{2}\begin{bmatrix}\phi^{\wedge}_e&\rho^{\wedge}_e\\0&\phi^{\wedge}_e\end{bmatrix}$

理论上来说，即使在优化之后，由于每条边给定的观测数据并不一致，误差通常也不见得近似于零，所以简单地把这里的$\mathcal{J}_r$设置为I会有一定的损失。 

了解雅可比求导后，剩下的部分就和普通的图优化一样了。简而言之，所有的位姿顶点和位姿——位姿边构成了一个图优化，本质上是一个最小二乘问题，优化变量为各个顶点的位姿，边来自于位姿观测约束。记$E$为所有边的集合，那么总体目标函数为：

$\min\limits_{\xi}\frac{1}{2}\sum\limits_{i,j\in \varepsilon}e^T_{ij}\Sigma^{-1}_{ij}e_{ij}$

我们依然可以用Gauss-Newton、Levenberg-Marquardt等方法求解此问题，除了用李代数表示优化位姿以外，别的都是相似的。根据先前的经验，这自然可以用Ceres或g2o进行求解。

## 回环检测

### 概述

#### 回环检测的意义

前端提供特征点的提取和轨迹、地图的初值，而后端负责对这所有的数据进行优化。然而，如果像VO那样仅考虑相邻时间上的关联，那么，之前产生的误差将不可避免地累计到下一个时刻，使得整个SLAM会出现累积误差。长期估计的结果将不可靠，或者说，我们无法构建全局一致的轨迹和地图。 

举例来说，假设我们在前端提取了特征，然后忽略掉特征点，在后端使用Pose Graph优化整个轨迹，如下图a所示。由于前端给出的只是局部的位姿间约束，比方说，可能是$x_1−x_2$、$x_2−x_3$等等。但是，由于$x_1$的估计存在误差，而$x_2$是根据$x_1$决定的，$x_3$又是由$x_2$决定的。以此类推，误差就会被累积起来，使得后端优化的结果如下图b所示，慢慢地趋向不准确。

虽然后端能够估计最大后验误差，但所谓"好模型架不住烂数据"，只有相邻关键帧数据时，我们能做的事情并不很多，也无从消除累积误差。但是，回环检测模块，能够给出除了相邻帧之外的，一些时隔更加久远的约束：例如$x_1−x_100$之间的位姿变换。为什么它们之间会有约束呢？这是因为我们察觉到相机经过了同一个地方，采集到了相似的数据。而回环检测的关键，就是如何有效地检测出相机经过同一个地方这件事。如果我们能够成功地检测这件事，就可以为后端的Pose Graph提供更多的有效数据，使之得到更好的估计，特别是得到一个全局一致（Global Consistent）的估计。由于Pose Graph可以看成一个质点-弹簧系统，所以回环检测相当于在图像中加入了额外的弹簧，提高了系统稳定性。回环边把带有累计误差的边"拉"到了正确的位置，如果回环本身是正确的话。 

回环检测对于SLAM系统意义重大。它关系到我们估计的轨迹和地图在长时间下的正确性。另一方面，由于回环检测提供了当前数据与所有历史数据的关联，在跟踪算法丢失之后，我们还可以利用回环检测进行重定位。因此，回环检测对整个SLAM系统精度与鲁棒性的提升，是非常明显的。甚至在某些时候，我们把仅有前端和局部后端的系统称为VO，而把带有回环检测和全局后端的称为SLAM。

#### 方法

回环检测最简单的实现方式就是对任意两张图像都做一遍特征匹配，根据正确匹配的数量确定哪两个图像存在关联。这确实是一种朴素且有效的思想。缺点在于，我们盲目地假设了"任意两个图像都可能存在回环"，使得要检测的数量实在太大：对于$N$个可能的回环，我们要检测$C^2_N$那么多次，这是$O(N^2)$的复杂度，随着轨迹变长增长太快，在大多数实时系统当中是不实用的。另一种朴素的方式是，随机抽取历史数据并进行回环检测，比如说在$N$帧当中随机抽$5$帧与当前帧比较。这种做法能够维持常数时间的运算量，但是这种盲目试探方法在帧数$N$增长时，抽到回环的几率又大幅下降，使得检测效率不高。 

上面说的朴素思路都过于粗糙。尽管随机检测在有些实现中确实有用，但我们至少希望有一个"哪处可能出现回环"的预计，才好不那么盲目地去检测。这样的方式大体分为两种思路：基于里程计的几何关系（Odometry based），或基于外观（Appearance based）。 

基于几何关系是说，当我们发现当前相机运动到了之前的某个位置附近时，检测它们有没有回环关系,这自然是一种直观的想法，但是由于累积误差的存在，我们往往没法正确地发现"运动到了之前的某个位置附近"这件事实，回环检测也无从谈起。因此，这种做法在逻辑上存在一点问题，因为回环检测的目标在于发现"相机回到之前位置"的事 

实，从而消除累计误差。而基于几何关系的做法假设了"相机回到之前位置附近"，才能检测回环。这是有倒果为因的嫌疑的，因而也无法在累计误差较大时工作。 

另一种方法是基于外观的。它和前端后端的估计都无关，仅根据两张图像的相似性确定回环检测关系。这种做法摆脱了累计误差，使回环检测模块成为SLAM系统中一个相对独立的模块（当然前端可以为它提供特征点）。自21世纪初被提出以来，基于外观的回环检测方式能够有效地在不同场景下工作，成为了视觉SLAM中主流的做法，并被应用于实际的系统中去。 

在基于外观的回环检测算法中，核心问题是如何计算图像间的相似性。比如对于图像$A$和图像$B$，我们要设计一种方法，计算它们之间的相似性评分：$s(A,B)$。当然这个评分会在某个区间内取值，当它大于一定量后我们认为出现了一个回环。 

计算两个图像之间的相似性很困难吗？例如直观上看，图像能够表示成矩阵，那么直接让两个图像相减，然后取某种范数行不行呢：

$s(A,B)=\Vert A-B\Vert$

为什么我们不这样做？ 

1. 首先，像素灰度是一种不稳定的测量值，它严重受环境光照和相机曝光的影响。假设相机未动，我们打开了一支电灯，那么图像会整体变亮一些。这样，即使对于同样的数据，我们都会得到一个很大的差异值。 

2. 另一方面，当相机视角发生少量变化时，即使每个物体的光度不变，它们的像素也会在图像中发生位移，造成一个很大的差异值。 

由于这两种情况的存在，实际当中，即使对于非常相似的图像，$A−B$也会经常得到一个（不符合实际的）很大的值。所以我们说，这个函数不能很好的反映图像间的相似关系。这里牵涉到一个"好"和"不好"的定义问题。怎样的函数能够更好地反映相似关系，而怎样的函数不够好呢？从这里可以引出感知偏差（Perceptual Aliasing）和感知变异（Perceptual Variability）两个概念。

#### 准确率和召回率

从人类的角度看，（至少我们自认为）我们能够以很高的精确度，感觉到"两张图像是否相似"或"这两张照片是从同一个地方拍摄的"这件事实，但由于目前尚未知道人脑的工作原理，我们无法清楚地描述自己是如何完成这件事的。从程序角度看，我们希望程序算法能够得出和人类，或者和事实一致的判断。当我们觉得，或者事实上就是，两张图像从同一个地方拍摄，那么回环检测算法也应该给出"这是回环"的结果。反之，如果我们觉得，或事实上是，两张图像是从不同地方拍摄的，那么程序也应该给出"这不是回环"的判断。当然，程序的判断并不总是与人类想法一致，所以可能出现四种情况：

<img src="slambook/2025-04-05-11-59-16-2842470433b2ec839e546b346d1d4353.png" title="" alt="" width="472">

这里阴性/阳性的说法是借用了医学上的说法。假阳性（False Positive）又称为感知偏差，而假阴性（False Negative）称为感知变异。为方便书写，记缩写TP为True Positive，其余类推。由于我们希望算法和人类的判断一致，所以希望TP和TN要尽量的高，而FP和FN要尽可能的低。所以，对于某种特定算法，我们可以统计它在某个数据集上的TP、TN、FP、FN的出现次数，并计算两个统计量：准确率和召回率（Precision & Recall）。

$\rm{Precision=TP/(TP+FP),\quad Recall=TP/(TP+FN)}$

从公式字面意义上来看，准确率描述的是，算法提取的所有回环中，确实是真实回环的概率。而召回率则是说，在所有真实回环中，被正确检测出来的概率。为什么取这两个统计量呢？因为它们有一定的代表性，并且通常来说是一个矛盾。

<img title="" src="slambook/839585a7cadcbff14c23d5e60a334b623523aa45.png" alt="" width="415">

一个算法往往有许多的设置参数。比方说，当我们提高某个阈值时，算法可能变得更加"严格"，它检出更少的回环，使准确率得以提高。但同时，由于检出的数量变少了，许多原本是回环的地方就可能被漏掉了，导致召回率的下降。反之，如果我们选择更加宽松的配置，那么检出的回环数量将增加，得到更高的召回率，但其中可能混杂了一些不是回环的情况，于是准确率下降了。 

为了评价算法的好坏，我们会测试它在各种配置下的P和R值，然后做出一条Precision-Recall曲线。当用召回率为横轴，用准确率为纵轴时，我们会关心整条曲线偏向右上方的程度、100%准确率下的召回率，或者50%召回率时候的准确率，作为评价算法的指标。不过请注意，除去一些"天壤之别"的算法，我们通常不能一概而论算法A就是优于算法B的。我们可能说A在准确率较高时还有很好的召回率，而B在70%召回率的情况下还能保证较好的准确率，诸如此类的评价。 

值得一提的是，在SLAM中，我们对准确率要求更高，而对召回率则相对宽容一些。由于假阳性的（检测结果是而实际不是的）回环将在后端的Pose Graph中添加根本错误的边，有些时候会导致优化算法给出完全错误的结果。想象一下，如果SLAM程序错误地将所有的办公桌当成了同一张，那建出来的图会怎么样呢？你可能会看到走廓不直了，墙壁被交错在一起了，最后整个地图都失效了。而相比之下，召回率低一些，则顶多有部分的回环没有被检测到，地图可能受一些累积误差的影响——然而仅需一两次回环就可以完全消除它们了。所以说在选择回环检测算法时，我们更倾向于把参数设置地更严格一些，或者在检测之后再加上回环验证的步骤。 

那么，为什么不用A−B来计算相似性呢？我们会发现它的准确率和召回率都很差，可能出现大量的False Positive或False Negative的情况，所以说这样做"不好"。

### 词袋模型

既然直接用两张图像相减的方式不够好，那么我们需要一种更加可靠的方式。一种直观的思路是：为何不像VO那样特征点来做回环检测呢？和VO一样，我们对两个图像的特征点进行匹配，只要匹配数量大于一定值，就认为出现了回环。进一步，根据特征点匹配，我们还能计算出这两张图像之间的运动关系。当然这种做法会存在一些问题，例如特征的匹配会比较费时、当光照变化时特征描述可能不稳定等，但离我们要介绍的词袋模型已经很相近了。我们先来讲词袋的做法，再来讨论数据结构之类的实现细节。 

词袋，也就是Bag-of-Words（BoW），目的是用"图像上有哪几种特征"来描述一个图像。例如，如果某个照片，我们说里面有一个人、一辆车；而另一张则有两个人、一只狗。根据这样的描述，可以度量这两个图像的相似性。再具体一些，我们要做以下几件事： 

1. 确定"人、车、狗"等概念，对应于BoW中的"单词"（Word），许多单词放在一起，组成了"字典"（Dictionary）。 

2. 确定一张图像中，出现了哪些在字典中定义的概念，我们用单词出现的情况（或直方图）描述整张图像。这就把一个图像转换成了一个向量的描述。 

3. 比较上一步中的描述的相似程度。 

4. 以上面举的例子来说，首先我们通过某种方式，得到了一本"字典"。字典上记录了许多单词，每个单词都有一定意义，例如"人"、"车"、"狗"都是记录在字典中的单词，我们不妨记为$w_1$、$w_2$、$w_3$。然后，对于任意图像A，根据它们含有的单词，可记为：$A=1\cdot w_1+1\cdot w_2+0\cdot w_3$

字典是固定的，所以只要用$[1,1,0]^T$这个向量就可以表达A的意义。通过字典和单词，只需一个向量就可以描述整张图像了。该向量描述的是"图像是否含有某类特征"的信息，比单纯的灰度值更加稳定。又因为描述向量说的是"是否出现"，而不管它们"在哪出现"，所以与物体的空间位置和排列顺序无关，因此在相机发生少量运动时，只要物体仍在视野中出现，我们就仍然保证描述向量不发生变化。基于这种特性，我们称它为Bag-of-Words而不是什么List-of-Words，强调的是Words的有无，而无关其顺序。因此，可以说字典类似于单词的一个集合。 

同理，用$[2,0,1]^T$可以描述图像$B$。如果只考虑"是否出现"而不考虑数量的话，也可以是$[1,0,1]^T$，这时候这个向量就是二值的。于是，根据这两个向量，设计一定的计算方式，就能确定图像间的相似性了。当然如果对两个向量求差仍然有一些不同的做法，比如说对于$a,b\in \mathbb{R}^W$，可以计算：

$s(a,b)=1-\frac{1}{W}\Vert a-b\Vert_1$

其中范数取L1范数，即各元素绝对值之和。请注意在两个向量完全一样时，我们将得到1；完全相反时（$a$为$0$的地方$b$为$1$）得到$0$。这样就定义了两个描述向量的相似性，也就定义了图像之间的相似程度。

### 字典模型

字典由很多单词组成，而每一个单词代表了一个概念。一个单词与一个单独的特征点不同，它不是从单个图像上提取出来的，而是某一类特征的组合。所以，字典生成问题类似于一个聚类（Clustering）问题。 

聚类问题是无监督机器学习（Unsupervised ML）中一个特别常见的问题，用于让机器自行寻找数据中的规律的问题。BoW 的字典生成问题亦属于其中之一。首先，假设我们对大量的图像提取了特征点，比如说有N个。现在，我们想找一个有k个单词的字典，每个单词可以看作局部相邻特征点的集合，应该怎么做呢？这可以用经典的K-means算法解决。 

K-means是一个非常简单有效的方法，因此在无监督学习中广为使用，我们稍加介绍它的原理。简单来说，当我们有$N$个数据，想要归成$k$个类，那么用K-means来做，主要有以下几个步骤：

1. 随机选取$k$个中心点：$c_1,...,c_k$。

2. 对每一个样本，计算与每个中心点之间的距离，取最小的作为它的归类。

3. 重新计算每个的中心点。

4. 如果每个中心点都变化很小，则算法收敛，退出；否则返回$1$。

K-means的做法是朴素且简单有效的，不过也存在一些问题，例如需要指定聚类数量、随机选取中心点使得每次聚类结果都不相同以及一些效率上的问题。随后研究者们亦开发出层次聚类法、K-means++等算法以弥补它的不足，不过这都是后话，我们就不详细讨论了。总之，根据K-means，我们可以把已经提取的大量特征点聚类成一个含有k个单词的字典了。现在的问题，变为如何根据图像中某个特征点，查找字典中相应的单词？ 

仍然有朴素的思想：只要和每个单词进行比对，取最相似的那个就可以了——这当然是简单有效的做法。然而，考虑到字典的通用性x，我们通常会使用一个较大规模的字典，以保证当前使用环境中的图像特征都曾在字典里出现过，或至少有相近的表达。如果你觉得对10个单词一一比较不是什么麻烦事，但对于一万个呢？十万个呢？

<img title="" src="slambook/c62f5f4a5eaccc025a1795d127f5b93a331f2003.png" alt="" width="517">

这种O(n)的查找算法显然不是我们想要的。如果字典排过序，那么二分查找显然可以提升查找效率，达到对数级别的复杂度。而实践当中，我们可能会用更复杂的数据结构，例如Fabmap中的Chou-Liu tree等等。此处介绍另一种较为简单实用的树结构。 

使用一种k叉树来表达字典。它的思路很简单，类似于层次聚类，是K-means的直接扩展。假定我们有$N$个特征点，希望构建一个深度为$d$，每次分叉为$k$的树，那么做法如下：

1. 在根节点，用k-means把所有样本聚成$k$类（实际中为保证聚类均匀性，会使用k-means++）。这样得到了第一层。

2. 对第一层的每个节点，把属于该节点的样本再聚成$k$类，得到下一层。

3. 以此类推，最后得到叶子层。叶子层即为所谓的Words。

实际上，最终我们仍在叶子层构建了单词，而树结构中的中间节点仅供快速查找时使用。这样一个$k$分支，深度为$d$的树，可以容纳$kd$个单词。另一方面，在查找某个给定特征对应的单词时，只需将它与每个中间结点的聚类中心比较（一共$d$次），即可找到最后的单词，保证了对数级别的查找效率。

### 相似度计算

#### 理论计算

有了字典之后，给定任意特征$f_i$，只要在字典树中逐层查找，最后都能找到与之对应的单词$w_j$。当字典足够大时，我们可以认为$f_i$和$w_j$来自同一类物体（尽管没有理论上的保证，仅是在聚类意义下这样说）。那么，假设一张图像中提取了$N$个特征，找到这$N$个特征对应的单词之后，我们相当于拥有了该图像在单词列表中的分布，或者直方图。直观上说（或理想情况下），相当于是说"这张图里有一个人和一辆汽车"这样的意思了。根据Bag-of-Words的说法，不妨认为这是一个Bag。 

注意到这种做法中，我们对所有单词都是"一视同仁"的——有就是有，没有就是没有。这样做好不好呢？考虑到，不同的单词在区分性上的重要性并不相同。例如"的"、"是"这样的字可能在许许多多的句子中出现，我们无法根据它们判别句子的类型；但如果有"文档"、"足球"这样的单词，对判别句子的作用就更大一些，可以说它们提供了更多信息。所以概括的话，我们希望对单词的区分性或重要性加以评估，给它们不同的权值以起到更好的效果。 

在文本检索中，常用的一种做法称为TF-IDF（Term Frequency–Inverse Document Frequency），或译频率-逆文档频率。TF部分的思想是，某单词在一个图像中经常出现，它的区分度就高。另一方面，IDF的思想是，某单词在字典中出现的频率越低，则分类图像时区分度越高。 

在词袋模型中，在建立字典时可以考虑IDF部分。我们统计某个叶子节点$w_i$中的特征数量相对于所有特征数量的比例，作为IDF部分。假设所有特征数量为$n$，$w_i$数量为$n_i$，那么该单词的IDF为：

${\rm IDF}_i=\log\frac{n}{n_i}$

另一方面，TF部分则是指某个特征在单个图像中出现的频率。假设图像$A$中，单词$w_i$出现了$n_i$次，而一共出现的单词次数为$n$，那么TF为：

${\rm TF_i}=\frac{n_i}{n}$

于是$w_i$的权重等于TF乘IDF之积：

$\eta_i={\rm TF}_i\times {\rm IDF}_i$

考虑权重以后，对于某个图像$A$，它的特征点可对应到许多个单词，组成它的Bag-of-Words：

$A={(w_1,\eta_1),(w_2,\eta_2),...,(w_N,\eta_N)}\triangleq v_A$

由于相似的特征可能落到同一个类中，因此实际的$v_A$中会存在大量的零。无论如何，通过词袋，我们用单个向量$v_A$描述了一个图像$A$。这个向量$v_A$是一个稀疏的向量，它的非零部分指示出图像$A$中含有哪些单词，而这些部分的值为TF-IDF的值。 

接下来的问题是：给定$v_A$和$v_B$，如何计算它们的差异呢？这个问题和范数定义的方式一样，存在若干种解决方式，比如L1范数形式：

$s(v_A-v_B)=2\sum\limits_{i=i}^N|v_{Ai}|+|v_{Bi}|-|v_{Ai}-v_{Bi}|$

当然也有很多种别的方式，不过在这里我们仅举一例作为演示。至此，我们已说明了如何通过词袋模型来计算任意图像间的相似度了。

## 建图

### 概述

建图（Mapping），是SLAM的两大目标之一，因为SLAM被称为同时定位与建图。 

在经典的SLAM模型中，我们所谓的地图，即所有路标点的集合。一旦我们确定了路标点的位置，那就可以说我们完成了建图。于是，视觉里程计也好，Bundle Adjustment也好，事实上都建模了路标点的位置，并对它们进行优化。在这个角度上说，我们已经探讨了建图问题。 

由于人们对建图的需求不同。SLAM作为一种底层技术，往往是用来为上层应用提供信息的。如果上层是机器人，那么应用层的开发者可能希望使用SLAM来做全局的定位，并且让机器人在地图中导航。例如扫地机需要完成扫地工作，希望计算一条能够覆盖整张地图的路径。或者，如果上层是一个增强现实设备，那么开发者可能希望将虚拟物体叠加在现实物体之中，特别地，还可能需要处理虚拟物体和真实物体的遮挡关系。 

我们发现，应用层面对于"定位"的需求是相似的，他们希望SLAM提供相机或搭载相机的主体的空间位姿信息。而对于地图，则存在着许多不同的需求。在视觉SLAM看来，"建图"是服务于"定位"的；但是在应用层面看来，"建图"明显还带有许多其它需求。关于地图的用处，我们大致归纳如下： 

1. 定位。定位是地图的一个基本功能。我们可以利用局部地图来实现定位。或者，只要有全局的描述子信息，我们也能通过回环检测确定机器人的位置。更进一步，我们还希望能够把地图保存下来，让机器人在下次开机后依然能在地图中定位，这样只需对地图进行一次建模，而不是每次启动机器人都重新做一次完整的SLAM。 

2. 导航。导航是指机器人能够在地图中进行路径规划，从任意两个地图点间寻找路径，然后控制自己运动到目标点的过程。该过程中，我们至少需要知道地图中哪些地方不可通过，而哪些地方是可以通过的。这就超出了稀疏特征点地图的能力范围，我们必须有另外的地图形式。稍后我们会说，这至少得是一种稠密的地图。 

3. 避障。避障也是机器人经常碰到的一个问题。它与导航类似，但更注重局部的、动态的障碍物的处理。同样的，仅有特征点，我们无法判断某个特征点是否为障碍物，所以我们将需要稠密地图。 

4. 重建。有时候，我们希望利用SLAM获得周围环境的重建效果，并把它展示给其他人看。这种地图主要用于向人展示，所以我们希望它看上去比较舒服、美观。或者，我们也可以把该地图用于通讯，使其他人能够远程地观看我们重建得到的三维物体或场景，例如三维的视频通话或者网上购物等等。这种地图亦是稠密的，并且我们还对它的外观有一些要求。我们可能不满足于稠密点云重建，更希望能够构建带纹理的平面，就像电子游戏中的三维场景那样。 

5. 交互。交互主要指人与地图之间的互动。例如，在增强现实中，我们会在房间里放置虚拟的物体，并与这些虚拟物体之间有一些互动，比方说我会点击墙面上放着的虚拟网页浏览器来观看视频，或者向墙面投掷物体，希望它们有（虚拟的）物理碰撞。另一方面，机器人应用中也会有与人、与地图之间的交互。例如机器人可能会收到命令"取桌子上的报纸"，那么，除了有环境地图之外，机器人还需要知道哪一块地图是"桌子"，什么叫做"之上"，什么又叫做"报纸"。这需要机器人对地图有更高级层面的认知，亦称为语义地图。

![](slambook/07fc9a78eefc3acfacf02ff42a4b070540bfce6a.png)

上图形象地解释了上面讨论的各种地图类型与用途之间的关系。我们之前的讨论，基本集中于"稀疏路标地图"的部分，还没有探讨稠密地图。所谓稠密地图是相对于稀疏地图而言的，稀疏地图只建模感兴趣的部分，也就是前面说了很久的特征点（路标点）。而稠密地图是指，建模所有看到过的部分。对于同一个桌子，稀疏地图可能只建模了桌子的四个角，而稠密地图则会建模整个桌面。虽然从定位角度看，只有四个角的地图也可以用于对相机进行定位，但由于我们无法从四个角推断这几个点之间的空间结构，所以无法仅用四个角来完成导航、避障等需要稠密地图才能完成的工作。

### 单目稠密重建

#### 立体视觉

相机，很久以来被认为是只有角度的传感器（Bearing only）。单个图像中的像素，只能提供物体与相机成像平面的角度以及物体采集到的亮度，而无法提供物体的距离（Range）。而在稠密重建，我们需要知道每一个像素点（或大部分像素点）的距离，那么大致上有以下几种解决方案：

1. 使用单目相机，利用移动相机之后进行三角化，测量像素的距离。

2. 使用双目相机，利用左右目的视差计算像素的距离（多目原理相同）。

3. 使用RGB-D相机直接获得像素距离。

前两种方式称为立体视觉（Stereo Vision），其中移动单目的又称为移动视角的立体视觉（Moving View Stereo）。相比于RGB-D直接测量的深度，单目和双目对深度的获取往往是"费力不讨好"的，我们需要花费大量的计算，最后得到一些不怎么可靠的深度估计。当然，RGB-D也有一些量程、应用范围和光照的限制，不过相比于单目和双目的结果，使用RGB-D进行稠密重建往往是更常见的选择。而单目双目的好处，是在目前RGB-D还无法很好应用的室外、大场景场合中，仍能通过立体视觉估计深度信息。
不考虑SLAM，先来考虑稍为简单的建图问题。
假定有某一段视频序列，我们通过某种方法得到了每一帧对应的轨迹（当然也很可能是由视觉里程计前端估计所得）。现在我们以第一张图像为参考帧，计算参考帧中每一个像素的深度（或者说距离）。

首先，我们对图像提取特征，并根据描述子计算了特征之间的匹配。换言之，通过特征，我们对某一个空间点进行了跟踪，知道了它在各个图像之间的位置。

然后，由于我们无法仅用一张图像确定特征点的位置，所以必须通过不同视角下的观测，估计它的深度，原理即三角测量。
那么，在稠密深度图估计中，不同之处在于，我们无法把每个像素都当作特征点，计算描述子。因此，稠密深度估计问题中，匹配就成为很重要的一环：如何确定第一张图的某像素，出现在其他图里的位置呢？这需要用到极线搜索和块匹配技术。然后，当我们知道了某个像素在各个图中的位置，就能像特征点那样，利用三角测量确定它的深度。不过不同的是，在这里我们要使用很多次三角测量让深度估计收敛，而不仅是一次。我们希望深度估计，能够随着测量的增加，从一个非常不确定的量，逐渐收敛到一个稳定值。这就是深度滤波器技术。

#### 极线搜索与块匹配

<img title="" src="slambook/58a58a36dacd3f137fff32b677deaee11d832ea9.jpg" alt="" width="488">

我们先来探讨不同视角下观察同一个点，产生的几何关系。这非常像对极几何关系。请看上图。左边的相机观测到了某个像素$p_1$。由于这是一个单目相机，我们无从知道它的深度，所以假设这个深度可能在某个区域之内，不妨说是某最小值到无穷远之间：$(d_{min},+\infty)$。因此，该像素对应的空间点就分布在某条线段上。在另一个视角（右侧相机）看来，这条线段的投影也形成图像平面上的一条线，我们知道这称为极线。当我们知道两个相机间的运动时，这条极线也是能够确定的。那么问题就是：极线上的哪一个点，是我们刚才看到的p_1点呢？
在特征点方法中，我们通过特征匹配找到了$p_2$的位置。然而现在我们没有描述子，所以只能在极线上搜索和$p_1$长的比较相似的点。再具体地说，我们可能沿着第二张图像中的极线的某一头，走到另一头，逐个比较每个像素与$p_1$的相似程度。从直接比较像素的角度上来看，这种做法倒是和直接法是异曲同工的。
在直接法的讨论中我们也知道，比较单个像素的亮度值并不一定稳定可靠。一件很明显的事情就是：万一极线上有很多和$p_1$相似的点，我们怎么确定哪一个是真实的呢？这似乎回到了我们在回环检测当中说到的问题：如何确定两个图像（或两个点）的相似性？回环检测是通过词袋来解决的，但这里由于没有特征，所以我们只好寻求另外的途径。
一种直观的想法是：既然单个像素的亮度没有区分性，那是否可以比较像素块呢？我们在$p_1$周围取一个大小为$w\times w$的小块，然后在极线上也取很多同样大小的小块进行比较，就可以一定程度上提高区分性。这就是所谓的块匹配。注意到在这个过程中，只有我们的假设在不同图像间整个小块的灰度值不变，这种比较才有意义。所以算法的假设，从像素的灰度不变性，变成了图像块的灰度不变性，在一定程度上变得更强了。
好了，现在我们取了$p_1$周围的小块，并且在极线上也取了很多个小块。不妨把$p_1$周围的小块记成$A\in\mathbb{R}^{w\times w}$把极线上的n个小块记成$B_i$，$i=1,...,n$。那么，如何计算小块与小块间的差异呢？存在若干种不同的计算方法：

1. SAD（Sum of Absolute Difference）。顾名思义，即取两个小块的差的绝对值之和：
   
   $S(A,B)_{SAD}=\sum\limits_{i,j}|A(i,j)-B(i,j)|$

2. SSD（Sum of Squared Distance，平方和）：
   
   $S(A,B)_{SAD}=\sum\limits_{i,j}(A(i,j)-B(i,j))^2$

3. NCC（Normalized Cross Correlation，归一化互相关）。这种方式比前两者要复杂一些，它计算的是两个小块的相关性：
   
   $S(A,B)_{SAD}=\frac{\sum\limits_{i,j}A(i,j)B(i,j)}{\sqrt{\sum\limits_{i,j}A(i,j)^2\sum\limits_{i,j}B(i,j)^2}}$
   
   请注意，由于这里用的是相关性，所以相关性接近0表示两个图像不相似，而接近1才表示相似。前面两种距离则是反过来的，接近0表示相似，而大的数值表示不相似。

和我们遇到过的许多情形一样，这些计算方式往往存在一个精度-效率之间的矛盾。精度好的方法往往需要复杂的计算，而简单的快速算法又往往效果不佳。这需要我们在实际工程中进行取舍。另外，除了这些简单版本之外，我们可以先把每个小块的均值去掉，称为去均值的SSD、去均值的NCC等等。去掉均值之后，我们允许像"小块B比A整体上亮一些，但仍然很相似"这样的情况因此比之前的更加可靠一些。

#### 高斯分布的深度滤波器

对像素点深度的估计，本身亦可建模为一个状态估计问题，于是就自然存在滤波器与非线性优化两种求解思路。虽然非线性优化效果较好，但是在SLAM这种实时性要求较强的场合，考虑到前端已经占据了不少的计算量，建图方面则通常采用计算量较少的滤波器方式了。

对深度的分布假设存在着若干种不同的做法。首先，在比较简单的假设条件下，我们可以假设深度值服从高斯分布，得到一种类卡尔曼式的方法。

<img title="" src="slambook/4e59bb3ac0875cacc30e6ef4c3b8258e0150d6e9.jpg" alt="" width="325">

设某个像素点的深度$d$服从：

$P(d)=N(\mu^2)$

而每当新的数据到来，我们都会观测到它的深度。同样的，假设这次观测亦是一个高斯分布：

$P(d_{obs})=N(\mu_{obs},\sigma_{obs}^2)$

于是，我们的问题是，如何使用观测的信息，更新原先$d$的分布。这正是一个信息融合问题。根据附录A，我们明白两个高斯分布的乘积依然是一个高斯分布。设融合后的$d$的分布为$N(\mu_{fuse},\sigma_{fuse}^2)$，那么根据高斯分布的乘积，有：

$\mu_{fuse}=\frac{\sigma_{obs}^2\mu+\sigma^2\mu_{obs}}{\sigma^2+\sigma_{obs}^2}$

由于我们仅有观测方程而没有运动方程，所以这里深度仅用到了信息融合部分，而无须像完整的卡尔曼那样进行预测和更新。可以看到融合的方程确实比较浅显易懂，不过问题仍然存在：如何确定我们观测到深度的分布呢？即，如何计算$\mu_{obs}$、$\sigma_{obs}$呢？

关于$\mu_{obs}、\sigma_{obs}$，亦存在一些不同的处理方式。有的地方考虑了几何不确定性和光度不确定性二者之和，有的则仅考虑几何不确定性。我们暂时只考虑由几何关系带来的不确定性。现在，假设我们通过极线搜索和块匹配，确定了参考帧某个像素在当前帧的投影位置。那么，这个位置对深度的不确定性有多大呢？

<img title="" src="slambook/289baf4c718b12b8d042531d6072ab7c2cc5c579.jpg" alt="" width="509">

以上图为例。考虑某次极线搜索，我们找到了$p_1$对应的$p_2$点，从而观测到了$p_1$的深度值，认为$p_1$对应的三维点为$P$。从而，可记$O_1P$为$p$，$O_1O_2$为相机的平移$t$，$O_2P$记为$a$。并且，把这个三角形的下面两个角记作$\alpha$、$\beta$。现在，考虑极线$l_2$上存在着一个像素大小的误差，使得$\beta$角变成了$\beta'$，而$p$也变成了$p'$，并记上面那个角为$\gamma$。我们要问的是，这一个像素的误差，会导致$p'$与$p$产生多大的差距呢？

这是一个典型的几何问题。我们来列写这个量之间的几何关系。显然有：

$a=p-t$

$\alpha=\arccos(p,t)$

$\beta=\arccos(a,-t)$

对$p2$扰动一个像素，将使得$\beta$产生一个变化量$\delta\beta$，由于相机焦距为$f$，于是：

$\delta\beta=\arctan\frac{1}{f}$

所以

$\beta'=\beta+\delta\beta$

$\gamma=\pi-\alpha-\beta'$

于是，由正弦定理，$p'$的大小可以求得：

$\Vert p'\Vert=\Vert t\Vert\frac{\sin\beta'}{\sin\gamma}$

由此，我们确定了由单个像素的不确定引起的深度不确定性。如果认为极线搜索的块匹配仅有一个像素的误差，那么就可以设：

$\sigma_{obs}=\Vert p\Vert-\Vert p'\Vert$

当然，如果极线搜索的不确定性大于一个像素，我们亦可按照此推导来放大这个不确定性。在实际工程中，当不确定性小于一定阈值之后，就可以认为深度数据已经收敛了。

综上所述，我们给出了估计稠密深度的一个完整的过程：

1. 假设所有像素的深度满足某个初始的高斯分布；

2. 当新数据产生时，通过极线搜索和块匹配确定投影点位置；

3. 根据几何关系计算三角化后的深度以及不确定性；

4. 将当前观测融合进上一次的估计中。若收敛则停止计算，否则返回2。

### RGB-D稠密建图

除了使用单目和双目进行稠密重建之外，在适用范围内，RGB-D相机是一种更好的选择。在上一章中详细讨论的深度估计问题，在RGB-D相机中可以完全通过传感器中硬件测量得到，无需消耗大量的计算资源来估计它们。并且，RGB-D的结构光或飞时原理，保证了深度数据对纹理的无关性。即使面对纯色的物体，只要它能够反射光，我们就能测量到它的深度。这亦是RGB-D传感器的一大优势。

利用RGB-D进行稠密建图是相对容易的。不过，根据地图形式不同，也存在着若干种不同的主流建图方式。最直观最简单的方法，就是根据估算的相机位姿，将RGB-D数据转化为点云（Point Cloud），然后进行拼接，最后得到一个由离散的点组成的点云地图（Point Cloud Map）。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，可以使用三角网格（Mesh），面片（Surfel）进行建图。另一方面，如果希望知道地图的障碍物信息并在地图上导航，亦可通过体素（Voxel）建立占据网格地图（Occupancy Map）。

#### 点云地图

所谓点云，就是由一组离散的点表示的地图。最基本的点包含x、y、z三维坐标，也可以带有r、g、b的彩色信息。由于RGB-D相机提供了彩色图和深度图，很容易根据相机内参来计算RGB-D点云。如果通过某种手段，得到了相机的位姿，那么只要直接把点云进行加和，就可以获得全局的点云。在实际建图当中，我们还会对点云加一些滤波处理（外点去除滤波器、降采样滤波器等），获得更好的视觉效果。

我们的思路如下：

1. 在生成每帧点云时，去掉深度值太大或无效的点。这主要是考虑到Kinect的有效量
   程，超过量程之后的深度值会有较大误差。
2. 利用统计滤波器方法去除孤立点。该滤波器统计每个点与它最近N个点的距离值的
   分布，去除距离均值过大的点。这样，我们保留了那些"粘在一起"的点，去掉了孤
   立的噪声点。
3. 最后，利用体素滤波器（Voxel Filter）进行降采样。由于多个视角存在视野重叠，在
   重叠区域会存在大量的位置十分相近的点。这会无益地占用许多内存空间。体素滤波
   保证在某个一定大小的立方体（或称体素）内仅有一个点，相当于对三维空间进行了
   降采样，从而节省了很多存储空间。

点云地图为我们提供了比较基本的可视化地图，让我们能够大致了解环境的样子。它以三维方式存储，使得我们能够快速地浏览场景的各个角落，乃至在场景中进行漫游。点云的一大优势是可以直接由RGB-D图像高效地生成，不需要额外的处理。它的滤波操作也非常直观，且处理效率尚能接受。

不过，使用点云表达地图仍然是十分初级的，我们不妨按照对地图的需求，看看点云地图是否能满足：

1. 定位需求：取决于前端视觉里程计的处理方式。如果是基于特征点的视觉里程计，由于点云中没有存储特征点信息，所以无法用于基于特征点的定位方法。如果前端是点云的 ICP，那么可以考虑将局部点云对全局点云进行ICP以估计位姿。然而，这要求全局点云具有较好的精度。在我们这种处理点云的方式中，并没有对点云本身进行优化，所以是不够的。
2. 导航与避障的需求：无法直接用于导航和避障。纯粹的点云无法表示"是否有障碍物"的信息，我们也无法在点云中做"任意空间点是否被占据"这样的查询，而这是导航和避障的基本需要。不过，可以在点云基础上进行加工，得到更适合导航与避障的地图形式。
3. 可视化和交互：具有基本的可视化与交互能力。我们能够看到场景的外观，也能在场景里漫游。从可视化角度来说，由于点云只含有离散的点，而没有物体表面信息（例如法线），所以不太符合人们对可视化习惯。例如，点云地图的物体从正面看和背面看是一样的，而且还能透过物体看到它背后的东西：这些都不符合我们日常的经验，因为我们没有物体表面的信息。

综上所述，我们说点云地图是"基础"的或"初级的"，是指它更接近于传感器读取的原始数据。它具有一些基本的功能，但通常用于调试和基本的显示，不便直接用于应用程序。如果我们希望地图有更高级的功能，点云地图是一个不错的出发点。例如，针对导航功能，我们可以从点云出发，构建占据网格地图（Occupancy Grid），以供导航算法查询某点是否可以通过。再如，SfM中常用的泊松重建方法，就能通过基本的点云重建物体网格地图，得到物体的表面信息。除泊松重建之外，Surfel亦是一种表达物体表面的方式，以面元作为地图的基本单位，能够建立漂亮的可视化地图。

#### 八叉树地图

八叉树是在导航中比较常用的，本身有较好的压缩性能的地图形式。

在点云地图中，我们虽然有了三维结构，亦进行了体素滤波以调整分辨率，但是点云有几个明显的缺陷：

- 点云地图通常规模很大，所以一个pcd文件也会很大。一张640×480的图像，会产生30万个空间点，需要大量的存储空间。即使经过一些滤波之后，pcd 文件也是很大的。而且讨厌之处在于，它的"大"并不是必需的。点云地图提供了很多不必要的细节。对于地毯上的褶皱、阴暗处的影子，我们并不特别关心这些东西。把它们放在地图里是浪费空间。由于这些空间的占用，除非我们降低分辨率，否则在有限的内存中，无法建模较大的环境。然而降低分辨率会导致地图质量下降。有没有什么方式对地图进行压缩地存储，舍弃一些重复的信息呢？

- 点云地图无法处理运动物体。因为我们的做法里只有"添加点"，而没有"当点消失时把它移除"的做法。而在实际环境中，运动物体的普遍存在，使得点云地图变得不够实用。

八叉树（Octo-map）就是一种灵活的、压缩的、又能随时更新的地图形式：

我们知道，把三维空间建模为许多个小方块（或体素），是一种常见的做法。如果我们把一个小方块的每个面平均切成两片，那么这个小方块就会变成同样大小的八个小方块。
这个步骤可以不断的重复，直到最后的方块大小达到建模的最高精度。在这个过程中，把"将一个小方块分成同样大小的八个"这件事，看成"从一个节点展开成八个子节点"，那么，整个从最大空间细分到最小空间的过程，就是一棵八叉树（Octo-tree）。

![](slambook/aa8e8797527f7e903098c9edb78798dde477b6a5.jpg)

左侧显示了一个大立方体不断地均匀分成八块，直到变成最小的方块为止。于是，整个大方块可以看成是根节点，而最小的块可以看作是"叶子节点"。于是，在八叉树中，当我们由下一层节点往上走一层时，地图的体积就能扩大八倍。
我们不妨做一点简单的计算：如果叶子节点的方块大小为$1\rm{cm^3}$，那么当我们限制八叉树为10层时，总共能建模的体积大约为$8^{10}=1073\rm{m^3}$，这足够建模一间屋子。由于体积与深度成指数关系，所以当我们用更大的深度时，建模的体积会增长的非常快。

在点云的体素滤波器中，我们不是也限制了一个体素中只有一个点吗？为何我们说点云占体积，而八叉树比较节省空间呢？这是因为，在八叉树中，我们在节点中存储它是否被占据的信息。然而，不一样之处，在于当某个方块的所有子节点都被占据或都不被占据时，就没必要展开这个节点。例如，一开始地图为空白时，我们就只需一个根节点，而不需要完整的树。当在地图中添加信息时，由于实际的物体经常连在一起，空白的地方也会常常连在一起，所以大多数八叉树节点都无需展开到叶子层面。所以说，八叉树比点云节省了大量的存储空间。

八叉树的节点存储了它是否被占据的信息。从点云层面来讲，我们自然可以用0表示空白，1表示被占据。这种0-1的表示可以用一个比特来存储，节省空间，不过显得有些过于简单了。由于噪声的影响，我们可能会看到某个点一会为0，一会儿为1；或者大部分时刻为0，小部分时刻为1；或者除了"是、否"两种情况之外，还有一个"未知"的状态。能否更精细地描述这件事呢？我们会选择用概率形式表达某节点是否被占据的事情。比方说，用一个浮点数$x\in[0,1]$来表达。这个$x$一开始取$0.5$。如果不断观测到它被占据，那么让这个值不断增加；反之，如果不断观测到它是空白，那就让它不断减小即可。

通过这种方式，我们动态地建模了地图中的障碍物信息。不过，现在的方式有一点小问题：如果让$x$不断增加或减小，它可能跑到$[0,1]$区间之外，带来处理上的不便。所以我们不是直接用概率来描述某节点被占据，而是用概率对数值（Log-odds）来描述。设$y\in\mathbb{R}$为概率对数值，$x$为$0$到$1$之间的概率，那么它们之间的变换由logit变换描述：

$y={\rm logit}(x)=\log(\frac{x}{1-x})$

其反变换为：

$x={\rm logit}^{-1}(y)=\frac{\exp(y)}{\exp(y)+1}$

可以看到，当$y$从$−\infty$变到$+\infty$时，$x$相应地从$0$变到了$1$。而当$y$取$0$时，$x$取到$0.5$。因此，我们不妨存储$y$来表达节点是否被占据。当不断观测到"占据"时，让$y$增加一个值；否则就让$y$减小一个值。当查询概率时，再用逆logit变换，将y转换至概率即可。用数学形式来说，设某节点为$n$，观测数据为$z$。那么从开始到$t$时刻某节点的概率对数值为$L(n|z_{1:t})$，那么$t+1$时刻为：

$L(n|z_{1:t+1})=L(n|z_{1:t-1})+L(n|z_t)$

如果写成概率形式而不是概率对数形式，就会有一点复杂：

$P(n|z_{1:T})=[1+\frac{1-P(n|z_T)}{P(n|z_T)}\frac{1-P(n|z_{T-1})}{P(n|z_{T-1})}\frac{P(n)}{1-P(n)}]^{-1}$

有了对数概率，我们就可以根据RGB-D数据，更新整个八叉树地图了。假设我们在RGB-D图像中观测到某个像素带有深度$d$，这说明了一件事：我们在深度值对应的空间点上观察到了一个占据数据，并且，从相机光心出发，到这个点的线段上，应该是没有物体的（否则会被遮挡）。利用这个信息，可以很好地对八叉树地图进行更新，并且能处理运动的结构。
