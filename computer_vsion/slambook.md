# slambook

## 建图

### 单目稠密重建

#### 立体视觉

相机，很久以来被认为是只有角度的传感器（Bearing only）。单个图像中的像素，只能提供物体与相机成像平面的角度以及物体采集到的亮度，而无法提供物体的距离（Range）。而在稠密重建，我们需要知道每一个像素点（或大部分像素点）的距离，那么大致上有以下几种解决方案：

1. 使用单目相机，利用移动相机之后进行三角化，测量像素的距离。

2. 使用双目相机，利用左右目的视差计算像素的距离（多目原理相同）。

3. 使用RGB-D相机直接获得像素距离。

前两种方式称为立体视觉（Stereo Vision），其中移动单目的又称为移动视角的立体视觉（Moving View Stereo）。相比于RGB-D直接测量的深度，单目和双目对深度的获取往往是"费力不讨好"的，我们需要花费大量的计算，最后得到一些不怎么可靠的深度估计。当然，RGB-D也有一些量程、应用范围和光照的限制，不过相比于单目和双目的结果，使用RGB-D进行稠密重建往往是更常见的选择。而单目双目的好处，是在目前RGB-D还无法很好应用的室外、大场景场合中，仍能通过立体视觉估计深度信息。
不考虑SLAM，先来考虑稍为简单的建图问题。
假定有某一段视频序列，我们通过某种方法得到了每一帧对应的轨迹（当然也很可能是由视觉里程计前端估计所得）。现在我们以第一张图像为参考帧，计算参考帧中每一个像素的深度（或者说距离）。

首先，我们对图像提取特征，并根据描述子计算了特征之间的匹配。换言之，通过特征，我们对某一个空间点进行了跟踪，知道了它在各个图像之间的位置。

然后，由于我们无法仅用一张图像确定特征点的位置，所以必须通过不同视角下的观测，估计它的深度，原理即三角测量。
那么，在稠密深度图估计中，不同之处在于，我们无法把每个像素都当作特征点，计算描述子。因此，稠密深度估计问题中，匹配就成为很重要的一环：如何确定第一张图的某像素，出现在其他图里的位置呢？这需要用到极线搜索和块匹配技术。然后，当我们知道了某个像素在各个图中的位置，就能像特征点那样，利用三角测量确定它的深度。不过不同的是，在这里我们要使用很多次三角测量让深度估计收敛，而不仅是一次。我们希望深度估计，能够随着测量的增加，从一个非常不确定的量，逐渐收敛到一个稳定值。这就是深度滤波器技术。

#### 极线搜索与块匹配

<img title="" src="file:///home/wushifan/notepad_yuki/computer_vsion/image/20250403-133708.jpg" alt="" width="407">

我们先来探讨不同视角下观察同一个点，产生的几何关系。这非常像对极几何关系。请看上图。左边的相机观测到了某个像素$p_1$。由于这是一个单目相机，我们无从知道它的深度，所以假设这个深度可能在某个区域之内，不妨说是某最小值到无穷远之间：$(d_{min},+\infty)$。因此，该像素对应的空间点就分布在某条线段上。在另一个视角（右侧相机）看来，这条线段的投影也形成图像平面上的一条线，我们知道这称为极线。当我们知道两个相机间的运动时，这条极线也是能够确定的。那么问题就是：极线上的哪一个点，是我们刚才看到的p_1点呢？
在特征点方法中，我们通过特征匹配找到了$p_2$的位置。然而现在我们没有描述子，所以只能在极线上搜索和$p_1$长的比较相似的点。再具体地说，我们可能沿着第二张图像中的极线的某一头，走到另一头，逐个比较每个像素与$p_1$的相似程度。从直接比较像素的角度上来看，这种做法倒是和直接法是异曲同工的。
在直接法的讨论中我们也知道，比较单个像素的亮度值并不一定稳定可靠。一件很明显的事情就是：万一极线上有很多和$p_1$相似的点，我们怎么确定哪一个是真实的呢？这似乎回到了我们在回环检测当中说到的问题：如何确定两个图像（或两个点）的相似性？回环检测是通过词袋来解决的，但这里由于没有特征，所以我们只好寻求另外的途径。
一种直观的想法是：既然单个像素的亮度没有区分性，那是否可以比较像素块呢？我们在$p_1$周围取一个大小为$w\times w$的小块，然后在极线上也取很多同样大小的小块进行比较，就可以一定程度上提高区分性。这就是所谓的块匹配。注意到在这个过程中，只有我们的假设在不同图像间整个小块的灰度值不变，这种比较才有意义。所以算法的假设，从像素的灰度不变性，变成了图像块的灰度不变性，在一定程度上变得更强了。
好了，现在我们取了$p_1$周围的小块，并且在极线上也取了很多个小块。不妨把$p_1$周围的小块记成$A\in\mathbb{R}^{w\times w}$把极线上的n个小块记成$B_i$，$i=1,...,n$。那么，如何计算小块与小块间的差异呢？存在若干种不同的计算方法：

1. SAD（Sum of Absolute Difference）。顾名思义，即取两个小块的差的绝对值之和：
   
   $S(A,B)_{SAD}=\sum\limits_{i,j}|A(i,j)-B(i,j)|$

2. SSD（Sum of Squared Distance，平方和）：
   
   $S(A,B)_{SAD}=\sum\limits_{i,j}(A(i,j)-B(i,j))^2$

3. NCC（Normalized Cross Correlation，归一化互相关）。这种方式比前两者要复杂一些，它计算的是两个小块的相关性：
   
   $S(A,B)_{SAD}=\frac{\sum\limits_{i,j}A(i,j)B(i,j)}{\sqrt{\sum\limits_{i,j}A(i,j)^2\sum\limits_{i,j}B(i,j)^2}}$
   
   请注意，由于这里用的是相关性，所以相关性接近0表示两个图像不相似，而接近1才表示相似。前面两种距离则是反过来的，接近0表示相似，而大的数值表示不相似。

和我们遇到过的许多情形一样，这些计算方式往往存在一个精度-效率之间的矛盾。精度好的方法往往需要复杂的计算，而简单的快速算法又往往效果不佳。这需要我们在实际工程中进行取舍。另外，除了这些简单版本之外，我们可以先把每个小块的均值去掉，称为去均值的SSD、去均值的NCC等等。去掉均值之后，我们允许像"小块B比A整体上亮一些，但仍然很相似"这样的情况因此比之前的更加可靠一些。

#### 高斯分布的深度滤波器

对像素点深度的估计，本身亦可建模为一个状态估计问题，于是就自然存在滤波器与非线性优化两种求解思路。虽然非线性优化效果较好，但是在SLAM这种实时性要求较强的场合，考虑到前端已经占据了不少的计算量，建图方面则通常采用计算量较少的滤波器方式了。

对深度的分布假设存在着若干种不同的做法。首先，在比较简单的假设条件下，我们可以假设深度值服从高斯分布，得到一种类卡尔曼式的方法。

<img src="file:///home/wushifan/notepad_yuki/computer_vsion/image/20250403-144317.jpg" title="" alt="" width="304">

设某个像素点的深度$d$服从：

$P(d)=N(\mu,\sigma^2)$

而每当新的数据到来，我们都会观测到它的深度。同样的，假设这次观测亦是一个高斯分布：

$P(d_{obs})=N(\mu_{obs},\sigma_{obs}^2)$

于是，我们的问题是，如何使用观测的信息，更新原先$d$的分布。这正是一个信息融合问题。根据附录A，我们明白两个高斯分布的乘积依然是一个高斯分布。设融合后的$d$的分布为$N(\mu_{fuse},\sigma_{fuse}^2)$，那么根据高斯分布的乘积，有：

$\mu_{fuse}=\frac{\sigma_{obs}^2\mu+\sigma^2\mu_{obs}}{\sigma^2+\sigma_{obs}^2}$

由于我们仅有观测方程而没有运动方程，所以这里深度仅用到了信息融合部分，而无须像完整的卡尔曼那样进行预测和更新。可以看到融合的方程确实比较浅显易懂，不过问题仍然存在：如何确定我们观测到深度的分布呢？即，如何计算$\mu_{obs}$、$\sigma_{obs}$呢？

关于$\mu_{obs}、\sigma_{obs}$，亦存在一些不同的处理方式。有的地方考虑了几何不确定性和光度不确定性二者之和，有的则仅考虑几何不确定性。我们暂时只考虑由几何关系带来的不确定性。现在，假设我们通过极线搜索和块匹配，确定了参考帧某个像素在当前帧的投影位置。那么，这个位置对深度的不确定性有多大呢？

![](/home/wushifan/notepad_yuki/computer_vsion/image/20250403-172049.jpg)

以上图为例。考虑某次极线搜索，我们找到了$p_1$对应的$p_2$点，从而观测到了$p_1$的深度值，认为$p_1$对应的三维点为$P$。从而，可记$O_1P$为$p$，$O_1O_2$为相机的平移$t$，$O_2P$记为$a$。并且，把这个三角形的下面两个角记作$\alpha$、$\beta$。现在，考虑极线$l_2$上存在着一个像素大小的误差，使得$\beta$角变成了$\beta'$，而$p$也变成了$p'$，并记上面那个角为$\gamma$。我们要问的是，这一个像素的误差，会导致$p'$与$p$产生多大的差距呢？

这是一个典型的几何问题。我们来列写这个量之间的几何关系。显然有：

$a=p-t$

$\alpha=\arccos(p,t)$

$\beta=\arccos(a,-t)$

对$p2$扰动一个像素，将使得$\beta$产生一个变化量$\delta\beta$，由于相机焦距为$f$，于是：

$\delta\beta=\arctan\frac{1}{f}$

所以

$\beta'=\beta+\delta\beta$

$\gamma=\pi-\alpha-\beta'$

于是，由正弦定理，$p'$的大小可以求得：

$\Vert p'\Vert=\Vert t\Vert\frac{\sin\beta'}{\sin\gamma}$

由此，我们确定了由单个像素的不确定引起的深度不确定性。如果认为极线搜索的块匹配仅有一个像素的误差，那么就可以设：

$\sigma_{obs}=\Vert p\Vert-\Vert p'\Vert$

当然，如果极线搜索的不确定性大于一个像素，我们亦可按照此推导来放大这个不确定性。在实际工程中，当不确定性小于一定阈值之后，就可以认为深度数据已经收敛了。

综上所述，我们给出了估计稠密深度的一个完整的过程：

1. 假设所有像素的深度满足某个初始的高斯分布；

2. 当新数据产生时，通过极线搜索和块匹配确定投影点位置；

3. 根据几何关系计算三角化后的深度以及不确定性；

4. 将当前观测融合进上一次的估计中。若收敛则停止计算，否则返回2。

### RGB-D稠密建图

除了使用单目和双目进行稠密重建之外，在适用范围内，RGB-D相机是一种更好的选择。在上一章中详细讨论的深度估计问题，在RGB-D相机中可以完全通过传感器中硬件测量得到，无需消耗大量的计算资源来估计它们。并且，RGB-D的结构光或飞时原理，保证了深度数据对纹理的无关性。即使面对纯色的物体，只要它能够反射光，我们就能测量到它的深度。这亦是RGB-D传感器的一大优势。

利用RGB-D进行稠密建图是相对容易的。不过，根据地图形式不同，也存在着若干种不同的主流建图方式。最直观最简单的方法，就是根据估算的相机位姿，将RGB-D数据转化为点云（Point Cloud），然后进行拼接，最后得到一个由离散的点组成的点云地图（Point Cloud Map）。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，可以使用三角网格（Mesh），面片（Surfel）进行建图。另一方面，如果希望知道地图的障碍物信息并在地图上导航，亦可通过体素（Voxel）建立占据网格地图（Occupancy Map）。

#### 点云地图

所谓点云，就是由一组离散的点表示的地图。最基本的点包含x、y、z三维坐标，也可以带有r、g、b的彩色信息。由于RGB-D相机提供了彩色图和深度图，很容易根据相机内参来计算RGB-D点云。如果通过某种手段，得到了相机的位姿，那么只要直接把点云进行加和，就可以获得全局的点云。在实际建图当中，我们还会对点云加一些滤波处理（外点去除滤波器、降采样滤波器等），获得更好的视觉效果。

我们的思路如下：

1. 在生成每帧点云时，去掉深度值太大或无效的点。这主要是考虑到Kinect的有效量
   程，超过量程之后的深度值会有较大误差。
2. 利用统计滤波器方法去除孤立点。该滤波器统计每个点与它最近N个点的距离值的
   分布，去除距离均值过大的点。这样，我们保留了那些"粘在一起"的点，去掉了孤
   立的噪声点。
3. 最后，利用体素滤波器（Voxel Filter）进行降采样。由于多个视角存在视野重叠，在
   重叠区域会存在大量的位置十分相近的点。这会无益地占用许多内存空间。体素滤波
   保证在某个一定大小的立方体（或称体素）内仅有一个点，相当于对三维空间进行了
   降采样，从而节省了很多存储空间。

点云地图为我们提供了比较基本的可视化地图，让我们能够大致了解环境的样子。它以三维方式存储，使得我们能够快速地浏览场景的各个角落，乃至在场景中进行漫游。点云的一大优势是可以直接由RGB-D图像高效地生成，不需要额外的处理。它的滤波操作也非常直观，且处理效率尚能接受。

不过，使用点云表达地图仍然是十分初级的，我们不妨按照对地图的需求，看看点云地图是否能满足：

1. 定位需求：取决于前端视觉里程计的处理方式。如果是基于特征点的视觉里程计，由于点云中没有存储特征点信息，所以无法用于基于特征点的定位方法。如果前端是点云的 ICP，那么可以考虑将局部点云对全局点云进行ICP以估计位姿。然而，这要求全局点云具有较好的精度。在我们这种处理点云的方式中，并没有对点云本身进行优化，所以是不够的。
2. 导航与避障的需求：无法直接用于导航和避障。纯粹的点云无法表示"是否有障碍物"的信息，我们也无法在点云中做"任意空间点是否被占据"这样的查询，而这是导航和避障的基本需要。不过，可以在点云基础上进行加工，得到更适合导航与避障的地图形式。
3. 可视化和交互：具有基本的可视化与交互能力。我们能够看到场景的外观，也能在场景里漫游。从可视化角度来说，由于点云只含有离散的点，而没有物体表面信息（例如法线），所以不太符合人们对可视化习惯。例如，点云地图的物体从正面看和背面看是一样的，而且还能透过物体看到它背后的东西：这些都不符合我们日常的经验，因为我们没有物体表面的信息。

综上所述，我们说点云地图是"基础"的或"初级的"，是指它更接近于传感器读取的原始数据。它具有一些基本的功能，但通常用于调试和基本的显示，不便直接用于应用程序。如果我们希望地图有更高级的功能，点云地图是一个不错的出发点。例如，针对导航功能，我们可以从点云出发，构建占据网格地图（Occupancy Grid），以供导航算法查询某点是否可以通过。再如，SfM中常用的泊松重建方法，就能通过基本的点云重建物体网格地图，得到物体的表面信息。除泊松重建之外，Surfel亦是一种表达物体表面的方式，以面元作为地图的基本单位，能够建立漂亮的可视化地图。

#### 八叉树地图

八叉树是在导航中比较常用的，本身有较好的压缩性能的地图形式。

在点云地图中，我们虽然有了三维结构，亦进行了体素滤波以调整分辨率，但是点云有几个明显的缺陷：

- 点云地图通常规模很大，所以一个pcd文件也会很大。一张640×480的图像，会产生30万个空间点，需要大量的存储空间。即使经过一些滤波之后，pcd 文件也是很大的。而且讨厌之处在于，它的"大"并不是必需的。点云地图提供了很多不必要的细节。对于地毯上的褶皱、阴暗处的影子，我们并不特别关心这些东西。把它们放在地图里是浪费空间。由于这些空间的占用，除非我们降低分辨率，否则在有限的内存中，无法建模较大的环境。然而降低分辨率会导致地图质量下降。有没有什么方式对地图进行压缩地存储，舍弃一些重复的信息呢？

- 点云地图无法处理运动物体。因为我们的做法里只有"添加点"，而没有"当点消失时把它移除"的做法。而在实际环境中，运动物体的普遍存在，使得点云地图变得不够实用。

八叉树（Octo-map）就是一种灵活的、压缩的、又能随时更新的地图形式：

我们知道，把三维空间建模为许多个小方块（或体素），是一种常见的做法。如果我们把一个小方块的每个面平均切成两片，那么这个小方块就会变成同样大小的八个小方块。
这个步骤可以不断的重复，直到最后的方块大小达到建模的最高精度。在这个过程中，把"将一个小方块分成同样大小的八个"这件事，看成"从一个节点展开成八个子节点"，那么，整个从最大空间细分到最小空间的过程，就是一棵八叉树（Octo-tree）。

![](/home/wushifan/notepad_yuki/computer_vsion/image/20250404-192907.jpg)

左侧显示了一个大立方体不断地均匀分成八块，直到变成最小的方块为止。于是，整个大方块可以看成是根节点，而最小的块可以看作是"叶子节点"。于是，在八叉树中，当我们由下一层节点往上走一层时，地图的体积就能扩大八倍。
我们不妨做一点简单的计算：如果叶子节点的方块大小为$1\rm{cm^3}$，那么当我们限制八叉树为10层时，总共能建模的体积大约为$8^{10}=1073\rm{m^3}$，这足够建模一间屋子。由于体积与深度成指数关系，所以当我们用更大的深度时，建模的体积会增长的非常快。

在点云的体素滤波器中，我们不是也限制了一个体素中只有一个点吗？为何我们说点云占体积，而八叉树比较节省空间呢？这是因为，在八叉树中，我们在节点中存储它是否被占据的信息。然而，不一样之处，在于当某个方块的所有子节点都被占据或都不被占据时，就没必要展开这个节点。例如，一开始地图为空白时，我们就只需一个根节点，而不需要完整的树。当在地图中添加信息时，由于实际的物体经常连在一起，空白的地方也会常常连在一起，所以大多数八叉树节点都无需展开到叶子层面。所以说，八叉树比点云节省了大量的存储空间。

八叉树的节点存储了它是否被占据的信息。从点云层面来讲，我们自然可以用0表示空白，1表示被占据。这种0-1的表示可以用一个比特来存储，节省空间，不过显得有些过于简单了。由于噪声的影响，我们可能会看到某个点一会为0，一会儿为1；或者大部分时刻为0，小部分时刻为1；或者除了"是、否"两种情况之外，还有一个"未知"的状态。能否更精细地描述这件事呢？我们会选择用概率形式表达某节点是否被占据的事情。比方说，用一个浮点数$x\in[0,1]$来表达。这个$x$一开始取$0.5$。如果不断观测到它被占据，那么让这个值不断增加；反之，如果不断观测到它是空白，那就让它不断减小即可。

通过这种方式，我们动态地建模了地图中的障碍物信息。不过，现在的方式有一点小问题：如果让$x$不断增加或减小，它可能跑到$[0,1]$区间之外，带来处理上的不便。所以我们不是直接用概率来描述某节点被占据，而是用概率对数值（Log-odds）来描述。设$y\in\mathbb{R}$为概率对数值，$x$为$0$到$1$之间的概率，那么它们之间的变换由logit变换描述：

$y=\rm{logit}(x)=\log(\frac{x}{1-x})$

其反变换为：

$x=\rm{logit}^{-1}(y)=\frac{\exp(y)}{\exp(y)+1}$

可以看到，当$y$从$−\infty$变到$+\infty$时，$x$相应地从$0$变到了$1$。而当$y$取$0$时，$x$取到$0.5$。因此，我们不妨存储$y$来表达节点是否被占据。当不断观测到"占据"时，让$y$增加一个值；否则就让$y$减小一个值。当查询概率时，再用逆logit变换，将y转换至概率即可。用数学形式来说，设某节点为$n$，观测数据为$z$。那么从开始到$t$时刻某节点的概率对数值为$L(n|z_{1:t})$，那么$t+1$时刻为：

$L(n|z_{1:t+1})=L(n|z_{1:t-1})+L(n|z_t)$

如果写成概率形式而不是概率对数形式，就会有一点复杂：

$P(n|z_{1:T})=[1+\frac{1-P(n|z_T)}{P(n|z_T)}\frac{1-P(n|z_{T-1})}{P(n|z_{T-1})}\frac{P(n)}{1-P(n)}]^{-1}$

有了对数概率，我们就可以根据RGB-D数据，更新整个八叉树地图了。假设我们在RGB-D图像中观测到某个像素带有深度$d$，这说明了一件事：我们在深度值对应的空间点上观察到了一个占据数据，并且，从相机光心出发，到这个点的线段上，应该是没有物体的（否则会被遮挡）。利用这个信息，可以很好地对八叉树地图进行更新，并且能处理运动的结构。
