# slambook

## 后端

### 概述

前端视觉里程计能给出一个短时间内的轨迹和地图，但由于不可避免的误差累积，这个地图在长时间内是不准确的。所以，在视觉里程计的基础上，我们还希望构建一个尺度、规模更大的优化问题，以考虑长时间内的最优轨迹和地图。不过，考虑到精度与性能的平衡，实际当中存在着许多不同的做法。

#### 状态估计的概率解释

视觉里程计只有短暂的记忆，而我们希望整个运动轨迹在较长时间内都能保持最优的状态。我们可能会用最新的知识，更新较久远之前的状态。站在"久远的状态"的角度上看，仿佛是未来的信息告诉它"你应该在哪里"。所以，在后端优化中，我 们通常考虑一个更长时间内（或所有时间内）的状态估计问题，而且不仅使用过去的信息更新自己的状态，也会用未来的信息来更新自己，这种处理方式不妨称为"批量的"（Batch）。 否则，如果当前的状态只由过去的时刻决定，甚至只由前一个时刻决定，那不妨称为"渐进的"（Incremental）。 

我们已经知道SLAM过程可以由运动方程和观测方程来描述。那么，假设在$t=0$到$t=N$的时间内，我们有$x_0$到$x_N$那么多个位姿，并且有$y_1,...,y_M$那么多个路标。运动和观测方程为：

$\begin{cases}x_k=f(x_k,u_k)+w_k\\z_{k,j}=h(y_i,x_k)+v_{k,j}\end{cases}\quad k=1,...,N,j=1,...,M$

注意以下几点： 

1. 观测方程中，只有当$x_k$看到了$y_j$时，才会产生观测数据，否则就没有。事实上，在一个位置通常只能看到一小部分路标。而且，由于视觉SLAM特征点数量众多，所以实际当中观测方程数量会远远大于运动方程的数量。 

2. 我们可能没有测量运动的装置，所以也可能没有运动方程。在这个情况下，有若干种处理方式：认为确实没有运动方程，或假设相机不动，或假设相机匀速运动。这几种方式都是可行的。在没有运动方程的情况下，整个优化问题就只由许多个观测方程组成。这就非常类似于SfM（Structure from Motion）问题，相当于我们通过一组图像来恢复运动和结构。与SfM中不同的是，SLAM中的图像有时间上的先后顺序，而SfM中允许使用完全无关的图像。

我们知道每个方程都受噪声影响，所以要把这里的位姿$x$和路标$y$看成服从某种概率分布的随机变量，而不是单独的一个数。因此，我们关心的问题就变成了：当我拥有某些运动数据$u$和观测数据$z$时，如何来确定状态量$x$、$y$的分布？进而，如果得到了新来时刻的数据之后，那么它们的分布又将发生怎样的变化？在比较常见且合理的情况下，我们假设状态量和噪声项服从高斯分布，意味着在程序中，只需要储存它们的均值和协方差矩阵即可。均值可看作是对变量最优值的估计，而协方差矩阵则度量了它的不确定性。那么，问题转变为：当存在一些运动数据和观测数据时，我们如何去估计状态量的高斯分布？ 

我们依然设身处地地扮演一下小萝卜。只有运动方程时，相当于我们蒙着眼睛在一个未知的地方走路。尽管我们知道自己每一步走了多远，但是随着时间增长，我们将对自己的位置越来越不确定，内心也就越加不安。这说明在输入数据受噪声影响时，我们对位置方差的估计将越来越大。但是，当我们睁开眼睛时，由于能够不断地观测到外部场景，使得位置估计的不确定性变小了，我们就会越来越自信。如果用椭圆或椭球直观地表达协方差阵，那么这个过程有点像是在手机地图软件中走路的感觉。以下图为例，当没有观测数据时，这个圆会随着运动越来越大；而如果有正确观测的话，圆就会缩小至一定的大小，保持稳定。不确定性的直观描述：左侧，只有运动方程时，由于下一个时刻的位姿是在上一个时刻基础上添加了噪声，所以不确定性越来越大；右侧，存在路标点（红色）时，不确定性会明显减小。不过请注意这只是一个直观的示意图，并非实际数据。

![](slambook/474e6e2b211c6be1cee33b07b1e3464331f127a1.png)

上面的过程以比喻的形式解释了状态估计中的问题，下面我们要以定量的方式来看待它，使用最大似然估计，把状态估计转换为最小二乘的做法。首先，由于位姿和路标点都是待估计的变量，我们改变一下记号，令$x_k$为$k$时刻的所有未知量。它包含了当前时刻的相机位姿与$m$个路标点。在这种记号的意义下，写成：



## 回环检测

### 概述

#### 回环检测的意义

前端提供特征点的提取和轨迹、地图的初值，而后端负责对这所有的数据进行优化。然而，如果像VO那样仅考虑相邻时间上的关联，那么，之前产生的误差将不可避免地累计到下一个时刻，使得整个SLAM会出现累积误差。长期估计的结果将不可靠，或者说，我们无法构建全局一致的轨迹和地图。 

举例来说，假设我们在前端提取了特征，然后忽略掉特征点，在后端使用Pose Graph优化整个轨迹，如下图a所示。由于前端给出的只是局部的位姿间约束，比方说，可能是$x_1−x_2$、$x_2−x_3$等等。但是，由于$x_1$的估计存在误差，而$x_2$是根据$x_1$决定的，$x_3$又是由$x_2$决定的。以此类推，误差就会被累积起来，使得后端优化的结果如下图b所示，慢慢地趋向不准确。

虽然后端能够估计最大后验误差，但所谓"好模型架不住烂数据"，只有相邻关键帧数据时，我们能做的事情并不很多，也无从消除累积误差。但是，回环检测模块，能够给出除了相邻帧之外的，一些时隔更加久远的约束：例如$x_1−x_100$之间的位姿变换。为什么它们之间会有约束呢？这是因为我们察觉到相机经过了同一个地方，采集到了相似的数据。而回环检测的关键，就是如何有效地检测出相机经过同一个地方这件事。如果我们能够成功地检测这件事，就可以为后端的Pose Graph提供更多的有效数据，使之得到更好的估计，特别是得到一个全局一致（Global Consistent）的估计。由于Pose Graph可以看成一个质点-弹簧系统，所以回环检测相当于在图像中加入了额外的弹簧，提高了系统稳定性。回环边把带有累计误差的边"拉"到了正确的位置，如果回环本身是正确的话。 

回环检测对于SLAM系统意义重大。它关系到我们估计的轨迹和地图在长时间下的正确性。另一方面，由于回环检测提供了当前数据与所有历史数据的关联，在跟踪算法丢失之后，我们还可以利用回环检测进行重定位。因此，回环检测对整个SLAM系统精度与鲁棒性的提升，是非常明显的。甚至在某些时候，我们把仅有前端和局部后端的系统称为VO，而把带有回环检测和全局后端的称为SLAM。

#### 方法

回环检测最简单的实现方式就是对任意两张图像都做一遍特征匹配，根据正确匹配的数量确定哪两个图像存在关联。这确实是一种朴素且有效的思想。缺点在于，我们盲目地假设了"任意两个图像都可能存在回环"，使得要检测的数量实在太大：对于$N$个可能的回环，我们要检测$C^2_N$那么多次，这是$O(N^2)$的复杂度，随着轨迹变长增长太快，在大多数实时系统当中是不实用的。另一种朴素的方式是，随机抽取历史数据并进行回环检测，比如说在$N$帧当中随机抽$5$帧与当前帧比较。这种做法能够维持常数时间的运算量，但是这种盲目试探方法在帧数$N$增长时，抽到回环的几率又大幅下降，使得检测效率不高。 

上面说的朴素思路都过于粗糙。尽管随机检测在有些实现中确实有用，但我们至少希望有一个"哪处可能出现回环"的预计，才好不那么盲目地去检测。这样的方式大体分为两种思路：基于里程计的几何关系（Odometry based），或基于外观（Appearance based）。 

基于几何关系是说，当我们发现当前相机运动到了之前的某个位置附近时，检测它们有没有回环关系,这自然是一种直观的想法，但是由于累积误差的存在，我们往往没法正确地发现"运动到了之前的某个位置附近"这件事实，回环检测也无从谈起。因此，这种做法在逻辑上存在一点问题，因为回环检测的目标在于发现"相机回到之前位置"的事 

实，从而消除累计误差。而基于几何关系的做法假设了"相机回到之前位置附近"，才能检测回环。这是有倒果为因的嫌疑的，因而也无法在累计误差较大时工作。 

另一种方法是基于外观的。它和前端后端的估计都无关，仅根据两张图像的相似性确定回环检测关系。这种做法摆脱了累计误差，使回环检测模块成为SLAM系统中一个相对独立的模块（当然前端可以为它提供特征点）。自21世纪初被提出以来，基于外观的回环检测方式能够有效地在不同场景下工作，成为了视觉SLAM中主流的做法，并被应用于实际的系统中去。 

在基于外观的回环检测算法中，核心问题是如何计算图像间的相似性。比如对于图像$A$和图像$B$，我们要设计一种方法，计算它们之间的相似性评分：$s(A,B)$。当然这个评分会在某个区间内取值，当它大于一定量后我们认为出现了一个回环。 

计算两个图像之间的相似性很困难吗？例如直观上看，图像能够表示成矩阵，那么直接让两个图像相减，然后取某种范数行不行呢：

$s(A,B)=\Vert A-B\Vert$

为什么我们不这样做？ 

1. 首先，像素灰度是一种不稳定的测量值，它严重受环境光照和相机曝光的影响。假设相机未动，我们打开了一支电灯，那么图像会整体变亮一些。这样，即使对于同样的数据，我们都会得到一个很大的差异值。 

2. 另一方面，当相机视角发生少量变化时，即使每个物体的光度不变，它们的像素也会在图像中发生位移，造成一个很大的差异值。 

由于这两种情况的存在，实际当中，即使对于非常相似的图像，$A−B$也会经常得到一个（不符合实际的）很大的值。所以我们说，这个函数不能很好的反映图像间的相似关系。这里牵涉到一个"好"和"不好"的定义问题。怎样的函数能够更好地反映相似关系，而怎样的函数不够好呢？从这里可以引出感知偏差（Perceptual Aliasing）和感知变异（Perceptual Variability）两个概念。

#### 准确率和召回率

从人类的角度看，（至少我们自认为）我们能够以很高的精确度，感觉到"两张图像是否相似"或"这两张照片是从同一个地方拍摄的"这件事实，但由于目前尚未知道人脑的工作原理，我们无法清楚地描述自己是如何完成这件事的。从程序角度看，我们希望程序算法能够得出和人类，或者和事实一致的判断。当我们觉得，或者事实上就是，两张图像从同一个地方拍摄，那么回环检测算法也应该给出"这是回环"的结果。反之，如果我们觉得，或事实上是，两张图像是从不同地方拍摄的，那么程序也应该给出"这不是回环"的判断。当然，程序的判断并不总是与人类想法一致，所以可能出现四种情况：

<img src="slambook/2025-04-05-11-59-16-2842470433b2ec839e546b346d1d4353.png" title="" alt="" width="472">

这里阴性/阳性的说法是借用了医学上的说法。假阳性（False Positive）又称为感知偏差，而假阴性（False Negative）称为感知变异。为方便书写，记缩写TP为True Positive，其余类推。由于我们希望算法和人类的判断一致，所以希望TP和TN要尽量的高，而FP和FN要尽可能的低。所以，对于某种特定算法，我们可以统计它在某个数据集上的TP、TN、FP、FN的出现次数，并计算两个统计量：准确率和召回率（Precision & Recall）。

$\rm{Precision=TP/(TP+FP),\quad Recall=TP/(TP+FN)}$

从公式字面意义上来看，准确率描述的是，算法提取的所有回环中，确实是真实回环的概率。而召回率则是说，在所有真实回环中，被正确检测出来的概率。为什么取这两个统计量呢？因为它们有一定的代表性，并且通常来说是一个矛盾。

<img title="" src="slambook/839585a7cadcbff14c23d5e60a334b623523aa45.png" alt="" width="415">

一个算法往往有许多的设置参数。比方说，当我们提高某个阈值时，算法可能变得更加"严格"，它检出更少的回环，使准确率得以提高。但同时，由于检出的数量变少了，许多原本是回环的地方就可能被漏掉了，导致召回率的下降。反之，如果我们选择更加宽松的配置，那么检出的回环数量将增加，得到更高的召回率，但其中可能混杂了一些不是回环的情况，于是准确率下降了。 

为了评价算法的好坏，我们会测试它在各种配置下的P和R值，然后做出一条Precision-Recall曲线。当用召回率为横轴，用准确率为纵轴时，我们会关心整条曲线偏向右上方的程度、100%准确率下的召回率，或者50%召回率时候的准确率，作为评价算法的指标。不过请注意，除去一些"天壤之别"的算法，我们通常不能一概而论算法A就是优于算法B的。我们可能说A在准确率较高时还有很好的召回率，而B在70%召回率的情况下还能保证较好的准确率，诸如此类的评价。 

值得一提的是，在SLAM中，我们对准确率要求更高，而对召回率则相对宽容一些。由于假阳性的（检测结果是而实际不是的）回环将在后端的Pose Graph中添加根本错误的边，有些时候会导致优化算法给出完全错误的结果。想象一下，如果SLAM程序错误地将所有的办公桌当成了同一张，那建出来的图会怎么样呢？你可能会看到走廓不直了，墙壁被交错在一起了，最后整个地图都失效了。而相比之下，召回率低一些，则顶多有部分的回环没有被检测到，地图可能受一些累积误差的影响——然而仅需一两次回环就可以完全消除它们了。所以说在选择回环检测算法时，我们更倾向于把参数设置地更严格一些，或者在检测之后再加上回环验证的步骤。 

那么，为什么不用A−B来计算相似性呢？我们会发现它的准确率和召回率都很差，可能出现大量的False Positive或False Negative的情况，所以说这样做"不好"。

### 词袋模型

既然直接用两张图像相减的方式不够好，那么我们需要一种更加可靠的方式。一种直观的思路是：为何不像VO那样特征点来做回环检测呢？和VO一样，我们对两个图像的特征点进行匹配，只要匹配数量大于一定值，就认为出现了回环。进一步，根据特征点匹配，我们还能计算出这两张图像之间的运动关系。当然这种做法会存在一些问题，例如特征的匹配会比较费时、当光照变化时特征描述可能不稳定等，但离我们要介绍的词袋模型已经很相近了。我们先来讲词袋的做法，再来讨论数据结构之类的实现细节。 

词袋，也就是Bag-of-Words（BoW），目的是用"图像上有哪几种特征"来描述一个图像。例如，如果某个照片，我们说里面有一个人、一辆车；而另一张则有两个人、一只狗。根据这样的描述，可以度量这两个图像的相似性。再具体一些，我们要做以下几件事： 

1. 确定"人、车、狗"等概念，对应于BoW中的"单词"（Word），许多单词放在一起，组成了"字典"（Dictionary）。 

2. 确定一张图像中，出现了哪些在字典中定义的概念，我们用单词出现的情况（或直方图）描述整张图像。这就把一个图像转换成了一个向量的描述。 

3. 比较上一步中的描述的相似程度。 

4. 以上面举的例子来说，首先我们通过某种方式，得到了一本"字典"。字典上记录了许多单词，每个单词都有一定意义，例如"人"、"车"、"狗"都是记录在字典中的单词，我们不妨记为$w_1$、$w_2$、$w_3$。然后，对于任意图像A，根据它们含有的单词，可记为：$A=1\cdot w_1+1\cdot w_2+0\cdot w_3$

字典是固定的，所以只要用$[1,1,0]^T$这个向量就可以表达A的意义。通过字典和单词，只需一个向量就可以描述整张图像了。该向量描述的是"图像是否含有某类特征"的信息，比单纯的灰度值更加稳定。又因为描述向量说的是"是否出现"，而不管它们"在哪出现"，所以与物体的空间位置和排列顺序无关，因此在相机发生少量运动时，只要物体仍在视野中出现，我们就仍然保证描述向量不发生变化。基于这种特性，我们称它为Bag-of-Words而不是什么List-of-Words，强调的是Words的有无，而无关其顺序。因此，可以说字典类似于单词的一个集合。 

同理，用$[2,0,1]^T$可以描述图像$B$。如果只考虑"是否出现"而不考虑数量的话，也可以是$[1,0,1]^T$，这时候这个向量就是二值的。于是，根据这两个向量，设计一定的计算方式，就能确定图像间的相似性了。当然如果对两个向量求差仍然有一些不同的做法，比如说对于$a,b\in \mathbb{R}^W$，可以计算：

$s(a,b)=1-\frac{1}{W}\Vert a-b\Vert_1$

其中范数取L1范数，即各元素绝对值之和。请注意在两个向量完全一样时，我们将得到1；完全相反时（$a$为$0$的地方$b$为$1$）得到$0$。这样就定义了两个描述向量的相似性，也就定义了图像之间的相似程度。

### 字典模型

字典由很多单词组成，而每一个单词代表了一个概念。一个单词与一个单独的特征点不同，它不是从单个图像上提取出来的，而是某一类特征的组合。所以，字典生成问题类似于一个聚类（Clustering）问题。 

聚类问题是无监督机器学习（Unsupervised ML）中一个特别常见的问题，用于让机器自行寻找数据中的规律的问题。BoW 的字典生成问题亦属于其中之一。首先，假设我们对大量的图像提取了特征点，比如说有N个。现在，我们想找一个有k个单词的字典，每个单词可以看作局部相邻特征点的集合，应该怎么做呢？这可以用经典的K-means算法解决。 

K-means是一个非常简单有效的方法，因此在无监督学习中广为使用，我们稍加介绍它的原理。简单来说，当我们有$N$个数据，想要归成$k$个类，那么用K-means来做，主要有以下几个步骤：

1. 随机选取$k$个中心点：$c_1,...,c_k$。

2. 对每一个样本，计算与每个中心点之间的距离，取最小的作为它的归类。

3. 重新计算每个的中心点。

4. 如果每个中心点都变化很小，则算法收敛，退出；否则返回$1$。

K-means的做法是朴素且简单有效的，不过也存在一些问题，例如需要指定聚类数量、随机选取中心点使得每次聚类结果都不相同以及一些效率上的问题。随后研究者们亦开发出层次聚类法、K-means++等算法以弥补它的不足，不过这都是后话，我们就不详细讨论了。总之，根据K-means，我们可以把已经提取的大量特征点聚类成一个含有k个单词的字典了。现在的问题，变为如何根据图像中某个特征点，查找字典中相应的单词？ 

仍然有朴素的思想：只要和每个单词进行比对，取最相似的那个就可以了——这当然是简单有效的做法。然而，考虑到字典的通用性x，我们通常会使用一个较大规模的字典，以保证当前使用环境中的图像特征都曾在字典里出现过，或至少有相近的表达。如果你觉得对10个单词一一比较不是什么麻烦事，但对于一万个呢？十万个呢？

<img title="" src="slambook/c62f5f4a5eaccc025a1795d127f5b93a331f2003.png" alt="" width="517">

这种O(n)的查找算法显然不是我们想要的。如果字典排过序，那么二分查找显然可以提升查找效率，达到对数级别的复杂度。而实践当中，我们可能会用更复杂的数据结构，例如Fabmap中的Chou-Liu tree等等。此处介绍另一种较为简单实用的树结构。 

使用一种k叉树来表达字典。它的思路很简单，类似于层次聚类，是K-means的直接扩展。假定我们有$N$个特征点，希望构建一个深度为$d$，每次分叉为$k$的树，那么做法如下：

1. 在根节点，用k-means把所有样本聚成$k$类（实际中为保证聚类均匀性，会使用k-means++）。这样得到了第一层。

2. 对第一层的每个节点，把属于该节点的样本再聚成$k$类，得到下一层。

3. 以此类推，最后得到叶子层。叶子层即为所谓的Words。

实际上，最终我们仍在叶子层构建了单词，而树结构中的中间节点仅供快速查找时使用。这样一个$k$分支，深度为$d$的树，可以容纳$kd$个单词。另一方面，在查找某个给定特征对应的单词时，只需将它与每个中间结点的聚类中心比较（一共$d$次），即可找到最后的单词，保证了对数级别的查找效率。

### 相似度计算

#### 理论计算

有了字典之后，给定任意特征$f_i$，只要在字典树中逐层查找，最后都能找到与之对应的单词$w_j$。当字典足够大时，我们可以认为$f_i$和$w_j$来自同一类物体（尽管没有理论上的保证，仅是在聚类意义下这样说）。那么，假设一张图像中提取了$N$个特征，找到这$N$个特征对应的单词之后，我们相当于拥有了该图像在单词列表中的分布，或者直方图。直观上说（或理想情况下），相当于是说"这张图里有一个人和一辆汽车"这样的意思了。根据Bag-of-Words的说法，不妨认为这是一个Bag。 

注意到这种做法中，我们对所有单词都是"一视同仁"的——有就是有，没有就是没有。这样做好不好呢？考虑到，不同的单词在区分性上的重要性并不相同。例如"的"、"是"这样的字可能在许许多多的句子中出现，我们无法根据它们判别句子的类型；但如果有"文档"、"足球"这样的单词，对判别句子的作用就更大一些，可以说它们提供了更多信息。所以概括的话，我们希望对单词的区分性或重要性加以评估，给它们不同的权值以起到更好的效果。 

在文本检索中，常用的一种做法称为TF-IDF（Term Frequency–Inverse Document Frequency），或译频率-逆文档频率。TF部分的思想是，某单词在一个图像中经常出现，它的区分度就高。另一方面，IDF的思想是，某单词在字典中出现的频率越低，则分类图像时区分度越高。 

在词袋模型中，在建立字典时可以考虑IDF部分。我们统计某个叶子节点$w_i$中的特征数量相对于所有特征数量的比例，作为IDF部分。假设所有特征数量为$n$，$w_i$数量为$n_i$，那么该单词的IDF为：

${\rm IDF}_i=\log\frac{n}{n_i}$

另一方面，TF部分则是指某个特征在单个图像中出现的频率。假设图像$A$中，单词$w_i$出现了$n_i$次，而一共出现的单词次数为$n$，那么TF为：

${\rm TF_i}=\frac{n_i}{n}$

于是$w_i$的权重等于TF乘IDF之积：

$\eta_i={\rm TF}_i\times {\rm IDF}_i$

考虑权重以后，对于某个图像$A$，它的特征点可对应到许多个单词，组成它的Bag-of-Words：

$A={(w_1,\eta_1),(w_2,\eta_2),...,(w_N,\eta_N)}\triangleq v_A$

由于相似的特征可能落到同一个类中，因此实际的$v_A$中会存在大量的零。无论如何，通过词袋，我们用单个向量$v_A$描述了一个图像$A$。这个向量$v_A$是一个稀疏的向量，它的非零部分指示出图像$A$中含有哪些单词，而这些部分的值为TF-IDF的值。 

接下来的问题是：给定$v_A$和$v_B$，如何计算它们的差异呢？这个问题和范数定义的方式一样，存在若干种解决方式，比如L1范数形式：

$s(v_A-v_B)=2\sum\limits_{i=i}^N|v_{Ai}|+|v_{Bi}|-|v_{Ai}-v_{Bi}|$

当然也有很多种别的方式，不过在这里我们仅举一例作为演示。至此，我们已说明了如何通过词袋模型来计算任意图像间的相似度了。

## 建图

### 概述

建图（Mapping），是SLAM的两大目标之一，因为SLAM被称为同时定位与建图。 

在经典的SLAM模型中，我们所谓的地图，即所有路标点的集合。一旦我们确定了路标点的位置，那就可以说我们完成了建图。于是，视觉里程计也好，Bundle Adjustment也好，事实上都建模了路标点的位置，并对它们进行优化。在这个角度上说，我们已经探讨了建图问题。 

由于人们对建图的需求不同。SLAM作为一种底层技术，往往是用来为上层应用提供信息的。如果上层是机器人，那么应用层的开发者可能希望使用SLAM来做全局的定位，并且让机器人在地图中导航。例如扫地机需要完成扫地工作，希望计算一条能够覆盖整张地图的路径。或者，如果上层是一个增强现实设备，那么开发者可能希望将虚拟物体叠加在现实物体之中，特别地，还可能需要处理虚拟物体和真实物体的遮挡关系。 

我们发现，应用层面对于"定位"的需求是相似的，他们希望SLAM提供相机或搭载相机的主体的空间位姿信息。而对于地图，则存在着许多不同的需求。在视觉SLAM看来，"建图"是服务于"定位"的；但是在应用层面看来，"建图"明显还带有许多其它需求。关于地图的用处，我们大致归纳如下： 

1. 定位。定位是地图的一个基本功能。我们可以利用局部地图来实现定位。或者，只要有全局的描述子信息，我们也能通过回环检测确定机器人的位置。更进一步，我们还希望能够把地图保存下来，让机器人在下次开机后依然能在地图中定位，这样只需对地图进行一次建模，而不是每次启动机器人都重新做一次完整的SLAM。 

2. 导航。导航是指机器人能够在地图中进行路径规划，从任意两个地图点间寻找路径，然后控制自己运动到目标点的过程。该过程中，我们至少需要知道地图中哪些地方不可通过，而哪些地方是可以通过的。这就超出了稀疏特征点地图的能力范围，我们必须有另外的地图形式。稍后我们会说，这至少得是一种稠密的地图。 

3. 避障。避障也是机器人经常碰到的一个问题。它与导航类似，但更注重局部的、动态的障碍物的处理。同样的，仅有特征点，我们无法判断某个特征点是否为障碍物，所以我们将需要稠密地图。 

4. 重建。有时候，我们希望利用SLAM获得周围环境的重建效果，并把它展示给其他人看。这种地图主要用于向人展示，所以我们希望它看上去比较舒服、美观。或者，我们也可以把该地图用于通讯，使其他人能够远程地观看我们重建得到的三维物体或场景，例如三维的视频通话或者网上购物等等。这种地图亦是稠密的，并且我们还对它的外观有一些要求。我们可能不满足于稠密点云重建，更希望能够构建带纹理的平面，就像电子游戏中的三维场景那样。 

5. 交互。交互主要指人与地图之间的互动。例如，在增强现实中，我们会在房间里放置虚拟的物体，并与这些虚拟物体之间有一些互动，比方说我会点击墙面上放着的虚拟网页浏览器来观看视频，或者向墙面投掷物体，希望它们有（虚拟的）物理碰撞。另一方面，机器人应用中也会有与人、与地图之间的交互。例如机器人可能会收到命令"取桌子上的报纸"，那么，除了有环境地图之外，机器人还需要知道哪一块地图是"桌子"，什么叫做"之上"，什么又叫做"报纸"。这需要机器人对地图有更高级层面的认知，亦称为语义地图。

![](slambook/07fc9a78eefc3acfacf02ff42a4b070540bfce6a.png)

上图形象地解释了上面讨论的各种地图类型与用途之间的关系。我们之前的讨论，基本集中于"稀疏路标地图"的部分，还没有探讨稠密地图。所谓稠密地图是相对于稀疏地图而言的，稀疏地图只建模感兴趣的部分，也就是前面说了很久的特征点（路标点）。而稠密地图是指，建模所有看到过的部分。对于同一个桌子，稀疏地图可能只建模了桌子的四个角，而稠密地图则会建模整个桌面。虽然从定位角度看，只有四个角的地图也可以用于对相机进行定位，但由于我们无法从四个角推断这几个点之间的空间结构，所以无法仅用四个角来完成导航、避障等需要稠密地图才能完成的工作。

### 单目稠密重建

#### 立体视觉

相机，很久以来被认为是只有角度的传感器（Bearing only）。单个图像中的像素，只能提供物体与相机成像平面的角度以及物体采集到的亮度，而无法提供物体的距离（Range）。而在稠密重建，我们需要知道每一个像素点（或大部分像素点）的距离，那么大致上有以下几种解决方案：

1. 使用单目相机，利用移动相机之后进行三角化，测量像素的距离。

2. 使用双目相机，利用左右目的视差计算像素的距离（多目原理相同）。

3. 使用RGB-D相机直接获得像素距离。

前两种方式称为立体视觉（Stereo Vision），其中移动单目的又称为移动视角的立体视觉（Moving View Stereo）。相比于RGB-D直接测量的深度，单目和双目对深度的获取往往是"费力不讨好"的，我们需要花费大量的计算，最后得到一些不怎么可靠的深度估计。当然，RGB-D也有一些量程、应用范围和光照的限制，不过相比于单目和双目的结果，使用RGB-D进行稠密重建往往是更常见的选择。而单目双目的好处，是在目前RGB-D还无法很好应用的室外、大场景场合中，仍能通过立体视觉估计深度信息。
不考虑SLAM，先来考虑稍为简单的建图问题。
假定有某一段视频序列，我们通过某种方法得到了每一帧对应的轨迹（当然也很可能是由视觉里程计前端估计所得）。现在我们以第一张图像为参考帧，计算参考帧中每一个像素的深度（或者说距离）。

首先，我们对图像提取特征，并根据描述子计算了特征之间的匹配。换言之，通过特征，我们对某一个空间点进行了跟踪，知道了它在各个图像之间的位置。

然后，由于我们无法仅用一张图像确定特征点的位置，所以必须通过不同视角下的观测，估计它的深度，原理即三角测量。
那么，在稠密深度图估计中，不同之处在于，我们无法把每个像素都当作特征点，计算描述子。因此，稠密深度估计问题中，匹配就成为很重要的一环：如何确定第一张图的某像素，出现在其他图里的位置呢？这需要用到极线搜索和块匹配技术。然后，当我们知道了某个像素在各个图中的位置，就能像特征点那样，利用三角测量确定它的深度。不过不同的是，在这里我们要使用很多次三角测量让深度估计收敛，而不仅是一次。我们希望深度估计，能够随着测量的增加，从一个非常不确定的量，逐渐收敛到一个稳定值。这就是深度滤波器技术。

#### 极线搜索与块匹配

<img title="" src="slambook/58a58a36dacd3f137fff32b677deaee11d832ea9.jpg" alt="" width="488">

我们先来探讨不同视角下观察同一个点，产生的几何关系。这非常像对极几何关系。请看上图。左边的相机观测到了某个像素$p_1$。由于这是一个单目相机，我们无从知道它的深度，所以假设这个深度可能在某个区域之内，不妨说是某最小值到无穷远之间：$(d_{min},+\infty)$。因此，该像素对应的空间点就分布在某条线段上。在另一个视角（右侧相机）看来，这条线段的投影也形成图像平面上的一条线，我们知道这称为极线。当我们知道两个相机间的运动时，这条极线也是能够确定的。那么问题就是：极线上的哪一个点，是我们刚才看到的p_1点呢？
在特征点方法中，我们通过特征匹配找到了$p_2$的位置。然而现在我们没有描述子，所以只能在极线上搜索和$p_1$长的比较相似的点。再具体地说，我们可能沿着第二张图像中的极线的某一头，走到另一头，逐个比较每个像素与$p_1$的相似程度。从直接比较像素的角度上来看，这种做法倒是和直接法是异曲同工的。
在直接法的讨论中我们也知道，比较单个像素的亮度值并不一定稳定可靠。一件很明显的事情就是：万一极线上有很多和$p_1$相似的点，我们怎么确定哪一个是真实的呢？这似乎回到了我们在回环检测当中说到的问题：如何确定两个图像（或两个点）的相似性？回环检测是通过词袋来解决的，但这里由于没有特征，所以我们只好寻求另外的途径。
一种直观的想法是：既然单个像素的亮度没有区分性，那是否可以比较像素块呢？我们在$p_1$周围取一个大小为$w\times w$的小块，然后在极线上也取很多同样大小的小块进行比较，就可以一定程度上提高区分性。这就是所谓的块匹配。注意到在这个过程中，只有我们的假设在不同图像间整个小块的灰度值不变，这种比较才有意义。所以算法的假设，从像素的灰度不变性，变成了图像块的灰度不变性，在一定程度上变得更强了。
好了，现在我们取了$p_1$周围的小块，并且在极线上也取了很多个小块。不妨把$p_1$周围的小块记成$A\in\mathbb{R}^{w\times w}$把极线上的n个小块记成$B_i$，$i=1,...,n$。那么，如何计算小块与小块间的差异呢？存在若干种不同的计算方法：

1. SAD（Sum of Absolute Difference）。顾名思义，即取两个小块的差的绝对值之和：
   
   $S(A,B)_{SAD}=\sum\limits_{i,j}|A(i,j)-B(i,j)|$

2. SSD（Sum of Squared Distance，平方和）：
   
   $S(A,B)_{SAD}=\sum\limits_{i,j}(A(i,j)-B(i,j))^2$

3. NCC（Normalized Cross Correlation，归一化互相关）。这种方式比前两者要复杂一些，它计算的是两个小块的相关性：
   
   $S(A,B)_{SAD}=\frac{\sum\limits_{i,j}A(i,j)B(i,j)}{\sqrt{\sum\limits_{i,j}A(i,j)^2\sum\limits_{i,j}B(i,j)^2}}$
   
   请注意，由于这里用的是相关性，所以相关性接近0表示两个图像不相似，而接近1才表示相似。前面两种距离则是反过来的，接近0表示相似，而大的数值表示不相似。

和我们遇到过的许多情形一样，这些计算方式往往存在一个精度-效率之间的矛盾。精度好的方法往往需要复杂的计算，而简单的快速算法又往往效果不佳。这需要我们在实际工程中进行取舍。另外，除了这些简单版本之外，我们可以先把每个小块的均值去掉，称为去均值的SSD、去均值的NCC等等。去掉均值之后，我们允许像"小块B比A整体上亮一些，但仍然很相似"这样的情况因此比之前的更加可靠一些。

#### 高斯分布的深度滤波器

对像素点深度的估计，本身亦可建模为一个状态估计问题，于是就自然存在滤波器与非线性优化两种求解思路。虽然非线性优化效果较好，但是在SLAM这种实时性要求较强的场合，考虑到前端已经占据了不少的计算量，建图方面则通常采用计算量较少的滤波器方式了。

对深度的分布假设存在着若干种不同的做法。首先，在比较简单的假设条件下，我们可以假设深度值服从高斯分布，得到一种类卡尔曼式的方法。

<img title="" src="slambook/4e59bb3ac0875cacc30e6ef4c3b8258e0150d6e9.jpg" alt="" width="325">

设某个像素点的深度$d$服从：

$P(d)=N(\mu^2)$

而每当新的数据到来，我们都会观测到它的深度。同样的，假设这次观测亦是一个高斯分布：

$P(d_{obs})=N(\mu_{obs},\sigma_{obs}^2)$

于是，我们的问题是，如何使用观测的信息，更新原先$d$的分布。这正是一个信息融合问题。根据附录A，我们明白两个高斯分布的乘积依然是一个高斯分布。设融合后的$d$的分布为$N(\mu_{fuse},\sigma_{fuse}^2)$，那么根据高斯分布的乘积，有：

$\mu_{fuse}=\frac{\sigma_{obs}^2\mu+\sigma^2\mu_{obs}}{\sigma^2+\sigma_{obs}^2}$

由于我们仅有观测方程而没有运动方程，所以这里深度仅用到了信息融合部分，而无须像完整的卡尔曼那样进行预测和更新。可以看到融合的方程确实比较浅显易懂，不过问题仍然存在：如何确定我们观测到深度的分布呢？即，如何计算$\mu_{obs}$、$\sigma_{obs}$呢？

关于$\mu_{obs}、\sigma_{obs}$，亦存在一些不同的处理方式。有的地方考虑了几何不确定性和光度不确定性二者之和，有的则仅考虑几何不确定性。我们暂时只考虑由几何关系带来的不确定性。现在，假设我们通过极线搜索和块匹配，确定了参考帧某个像素在当前帧的投影位置。那么，这个位置对深度的不确定性有多大呢？

<img title="" src="slambook/289baf4c718b12b8d042531d6072ab7c2cc5c579.jpg" alt="" width="509">

以上图为例。考虑某次极线搜索，我们找到了$p_1$对应的$p_2$点，从而观测到了$p_1$的深度值，认为$p_1$对应的三维点为$P$。从而，可记$O_1P$为$p$，$O_1O_2$为相机的平移$t$，$O_2P$记为$a$。并且，把这个三角形的下面两个角记作$\alpha$、$\beta$。现在，考虑极线$l_2$上存在着一个像素大小的误差，使得$\beta$角变成了$\beta'$，而$p$也变成了$p'$，并记上面那个角为$\gamma$。我们要问的是，这一个像素的误差，会导致$p'$与$p$产生多大的差距呢？

这是一个典型的几何问题。我们来列写这个量之间的几何关系。显然有：

$a=p-t$

$\alpha=\arccos(p,t)$

$\beta=\arccos(a,-t)$

对$p2$扰动一个像素，将使得$\beta$产生一个变化量$\delta\beta$，由于相机焦距为$f$，于是：

$\delta\beta=\arctan\frac{1}{f}$

所以

$\beta'=\beta+\delta\beta$

$\gamma=\pi-\alpha-\beta'$

于是，由正弦定理，$p'$的大小可以求得：

$\Vert p'\Vert=\Vert t\Vert\frac{\sin\beta'}{\sin\gamma}$

由此，我们确定了由单个像素的不确定引起的深度不确定性。如果认为极线搜索的块匹配仅有一个像素的误差，那么就可以设：

$\sigma_{obs}=\Vert p\Vert-\Vert p'\Vert$

当然，如果极线搜索的不确定性大于一个像素，我们亦可按照此推导来放大这个不确定性。在实际工程中，当不确定性小于一定阈值之后，就可以认为深度数据已经收敛了。

综上所述，我们给出了估计稠密深度的一个完整的过程：

1. 假设所有像素的深度满足某个初始的高斯分布；

2. 当新数据产生时，通过极线搜索和块匹配确定投影点位置；

3. 根据几何关系计算三角化后的深度以及不确定性；

4. 将当前观测融合进上一次的估计中。若收敛则停止计算，否则返回2。

### RGB-D稠密建图

除了使用单目和双目进行稠密重建之外，在适用范围内，RGB-D相机是一种更好的选择。在上一章中详细讨论的深度估计问题，在RGB-D相机中可以完全通过传感器中硬件测量得到，无需消耗大量的计算资源来估计它们。并且，RGB-D的结构光或飞时原理，保证了深度数据对纹理的无关性。即使面对纯色的物体，只要它能够反射光，我们就能测量到它的深度。这亦是RGB-D传感器的一大优势。

利用RGB-D进行稠密建图是相对容易的。不过，根据地图形式不同，也存在着若干种不同的主流建图方式。最直观最简单的方法，就是根据估算的相机位姿，将RGB-D数据转化为点云（Point Cloud），然后进行拼接，最后得到一个由离散的点组成的点云地图（Point Cloud Map）。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，可以使用三角网格（Mesh），面片（Surfel）进行建图。另一方面，如果希望知道地图的障碍物信息并在地图上导航，亦可通过体素（Voxel）建立占据网格地图（Occupancy Map）。

#### 点云地图

所谓点云，就是由一组离散的点表示的地图。最基本的点包含x、y、z三维坐标，也可以带有r、g、b的彩色信息。由于RGB-D相机提供了彩色图和深度图，很容易根据相机内参来计算RGB-D点云。如果通过某种手段，得到了相机的位姿，那么只要直接把点云进行加和，就可以获得全局的点云。在实际建图当中，我们还会对点云加一些滤波处理（外点去除滤波器、降采样滤波器等），获得更好的视觉效果。

我们的思路如下：

1. 在生成每帧点云时，去掉深度值太大或无效的点。这主要是考虑到Kinect的有效量
   程，超过量程之后的深度值会有较大误差。
2. 利用统计滤波器方法去除孤立点。该滤波器统计每个点与它最近N个点的距离值的
   分布，去除距离均值过大的点。这样，我们保留了那些"粘在一起"的点，去掉了孤
   立的噪声点。
3. 最后，利用体素滤波器（Voxel Filter）进行降采样。由于多个视角存在视野重叠，在
   重叠区域会存在大量的位置十分相近的点。这会无益地占用许多内存空间。体素滤波
   保证在某个一定大小的立方体（或称体素）内仅有一个点，相当于对三维空间进行了
   降采样，从而节省了很多存储空间。

点云地图为我们提供了比较基本的可视化地图，让我们能够大致了解环境的样子。它以三维方式存储，使得我们能够快速地浏览场景的各个角落，乃至在场景中进行漫游。点云的一大优势是可以直接由RGB-D图像高效地生成，不需要额外的处理。它的滤波操作也非常直观，且处理效率尚能接受。

不过，使用点云表达地图仍然是十分初级的，我们不妨按照对地图的需求，看看点云地图是否能满足：

1. 定位需求：取决于前端视觉里程计的处理方式。如果是基于特征点的视觉里程计，由于点云中没有存储特征点信息，所以无法用于基于特征点的定位方法。如果前端是点云的 ICP，那么可以考虑将局部点云对全局点云进行ICP以估计位姿。然而，这要求全局点云具有较好的精度。在我们这种处理点云的方式中，并没有对点云本身进行优化，所以是不够的。
2. 导航与避障的需求：无法直接用于导航和避障。纯粹的点云无法表示"是否有障碍物"的信息，我们也无法在点云中做"任意空间点是否被占据"这样的查询，而这是导航和避障的基本需要。不过，可以在点云基础上进行加工，得到更适合导航与避障的地图形式。
3. 可视化和交互：具有基本的可视化与交互能力。我们能够看到场景的外观，也能在场景里漫游。从可视化角度来说，由于点云只含有离散的点，而没有物体表面信息（例如法线），所以不太符合人们对可视化习惯。例如，点云地图的物体从正面看和背面看是一样的，而且还能透过物体看到它背后的东西：这些都不符合我们日常的经验，因为我们没有物体表面的信息。

综上所述，我们说点云地图是"基础"的或"初级的"，是指它更接近于传感器读取的原始数据。它具有一些基本的功能，但通常用于调试和基本的显示，不便直接用于应用程序。如果我们希望地图有更高级的功能，点云地图是一个不错的出发点。例如，针对导航功能，我们可以从点云出发，构建占据网格地图（Occupancy Grid），以供导航算法查询某点是否可以通过。再如，SfM中常用的泊松重建方法，就能通过基本的点云重建物体网格地图，得到物体的表面信息。除泊松重建之外，Surfel亦是一种表达物体表面的方式，以面元作为地图的基本单位，能够建立漂亮的可视化地图。

#### 八叉树地图

八叉树是在导航中比较常用的，本身有较好的压缩性能的地图形式。

在点云地图中，我们虽然有了三维结构，亦进行了体素滤波以调整分辨率，但是点云有几个明显的缺陷：

- 点云地图通常规模很大，所以一个pcd文件也会很大。一张640×480的图像，会产生30万个空间点，需要大量的存储空间。即使经过一些滤波之后，pcd 文件也是很大的。而且讨厌之处在于，它的"大"并不是必需的。点云地图提供了很多不必要的细节。对于地毯上的褶皱、阴暗处的影子，我们并不特别关心这些东西。把它们放在地图里是浪费空间。由于这些空间的占用，除非我们降低分辨率，否则在有限的内存中，无法建模较大的环境。然而降低分辨率会导致地图质量下降。有没有什么方式对地图进行压缩地存储，舍弃一些重复的信息呢？

- 点云地图无法处理运动物体。因为我们的做法里只有"添加点"，而没有"当点消失时把它移除"的做法。而在实际环境中，运动物体的普遍存在，使得点云地图变得不够实用。

八叉树（Octo-map）就是一种灵活的、压缩的、又能随时更新的地图形式：

我们知道，把三维空间建模为许多个小方块（或体素），是一种常见的做法。如果我们把一个小方块的每个面平均切成两片，那么这个小方块就会变成同样大小的八个小方块。
这个步骤可以不断的重复，直到最后的方块大小达到建模的最高精度。在这个过程中，把"将一个小方块分成同样大小的八个"这件事，看成"从一个节点展开成八个子节点"，那么，整个从最大空间细分到最小空间的过程，就是一棵八叉树（Octo-tree）。

![](slambook/aa8e8797527f7e903098c9edb78798dde477b6a5.jpg)

左侧显示了一个大立方体不断地均匀分成八块，直到变成最小的方块为止。于是，整个大方块可以看成是根节点，而最小的块可以看作是"叶子节点"。于是，在八叉树中，当我们由下一层节点往上走一层时，地图的体积就能扩大八倍。
我们不妨做一点简单的计算：如果叶子节点的方块大小为$1\rm{cm^3}$，那么当我们限制八叉树为10层时，总共能建模的体积大约为$8^{10}=1073\rm{m^3}$，这足够建模一间屋子。由于体积与深度成指数关系，所以当我们用更大的深度时，建模的体积会增长的非常快。

在点云的体素滤波器中，我们不是也限制了一个体素中只有一个点吗？为何我们说点云占体积，而八叉树比较节省空间呢？这是因为，在八叉树中，我们在节点中存储它是否被占据的信息。然而，不一样之处，在于当某个方块的所有子节点都被占据或都不被占据时，就没必要展开这个节点。例如，一开始地图为空白时，我们就只需一个根节点，而不需要完整的树。当在地图中添加信息时，由于实际的物体经常连在一起，空白的地方也会常常连在一起，所以大多数八叉树节点都无需展开到叶子层面。所以说，八叉树比点云节省了大量的存储空间。

八叉树的节点存储了它是否被占据的信息。从点云层面来讲，我们自然可以用0表示空白，1表示被占据。这种0-1的表示可以用一个比特来存储，节省空间，不过显得有些过于简单了。由于噪声的影响，我们可能会看到某个点一会为0，一会儿为1；或者大部分时刻为0，小部分时刻为1；或者除了"是、否"两种情况之外，还有一个"未知"的状态。能否更精细地描述这件事呢？我们会选择用概率形式表达某节点是否被占据的事情。比方说，用一个浮点数$x\in[0,1]$来表达。这个$x$一开始取$0.5$。如果不断观测到它被占据，那么让这个值不断增加；反之，如果不断观测到它是空白，那就让它不断减小即可。

通过这种方式，我们动态地建模了地图中的障碍物信息。不过，现在的方式有一点小问题：如果让$x$不断增加或减小，它可能跑到$[0,1]$区间之外，带来处理上的不便。所以我们不是直接用概率来描述某节点被占据，而是用概率对数值（Log-odds）来描述。设$y\in\mathbb{R}$为概率对数值，$x$为$0$到$1$之间的概率，那么它们之间的变换由logit变换描述：

$y={\rm logit}(x)=\log(\frac{x}{1-x})$

其反变换为：

$x={\rm logit}^{-1}(y)=\frac{\exp(y)}{\exp(y)+1}$

可以看到，当$y$从$−\infty$变到$+\infty$时，$x$相应地从$0$变到了$1$。而当$y$取$0$时，$x$取到$0.5$。因此，我们不妨存储$y$来表达节点是否被占据。当不断观测到"占据"时，让$y$增加一个值；否则就让$y$减小一个值。当查询概率时，再用逆logit变换，将y转换至概率即可。用数学形式来说，设某节点为$n$，观测数据为$z$。那么从开始到$t$时刻某节点的概率对数值为$L(n|z_{1:t})$，那么$t+1$时刻为：

$L(n|z_{1:t+1})=L(n|z_{1:t-1})+L(n|z_t)$

如果写成概率形式而不是概率对数形式，就会有一点复杂：

$P(n|z_{1:T})=[1+\frac{1-P(n|z_T)}{P(n|z_T)}\frac{1-P(n|z_{T-1})}{P(n|z_{T-1})}\frac{P(n)}{1-P(n)}]^{-1}$

有了对数概率，我们就可以根据RGB-D数据，更新整个八叉树地图了。假设我们在RGB-D图像中观测到某个像素带有深度$d$，这说明了一件事：我们在深度值对应的空间点上观察到了一个占据数据，并且，从相机光心出发，到这个点的线段上，应该是没有物体的（否则会被遮挡）。利用这个信息，可以很好地对八叉树地图进行更新，并且能处理运动的结构。
